<!-- .slide: class="center" -->

## 5. 高效注意力与架构改进

*每个字都仔细看虽好，可碰到长文本就太慢了！*

----

## 注意力机制的瓶颈

还记得吗？注意力机制让每个字去“扫一眼”所有其他字。这意味着：

- 10 个字 → 10 × 10 = **100** 次比较
- 1,000 个字 → 1,000 × 1,000 = **100 万** 次比较
- 100,000 个字 → 100,000 × 100,000 = **100 亿** 次比较！

文本越长，计算量就猛增——长度翻一倍，工作量翻四倍！有没有办法解决？

----

## FlashAttention：数学不变，换个搬法

诀窍不在于改公式，而在于改 **数据搬运的方式** 。

- GPU 里有“快但小”和“慢但大”两种内存
- 普通注意力要在两者之间来回搬数据，效率很低
- FlashAttention 重新编排了计算顺序，把数据搬运次数压到最少

计算结果完全一样，速度却 **快了 2-4 倍** ，显存占用也大幅降低！

> [FlashAttention](https://arxiv.org/abs/2205.14135)（2022）| [FlashAttention-2](https://arxiv.org/abs/2307.08691)（2023）

----

## GQA：大家一起用，别各存各的

标准注意力里，每个“注意力头”都有自己的一本笔记本来记住看过的内容。

32 个头就得存 32 份，太浪费了！

- **分组查询注意力 (GQA)** ：让好几个头 **共用** 一份缓存
- 32 个头只需 8 本笔记本 → 省了 4 倍的记忆空间
- 效果却几乎没有下降

> [GQA](https://arxiv.org/abs/2305.13245)（2023）— Llama 2、Mistral 等几乎所有主流大模型都在用

----

## RoPE：用“旋转”来表示位置

还记得前面说的位置编码——给每个字安排“座位号”吗？

最早的位置编码是固定的——训练时只见过 512 个字，所以只有 512 个“座位号”。一封短邮件（约 200 字）没问题，但《哈利·波特》 一章有约 5,000 字，几千个字根本没座位——模型直接懵掉。

- RoPE 的做法是给词向量施加一个 **旋转变换** 来编码位置
- 临近的字旋转角度相近，远处的字旋转角度差异大
- 好处是训练完之后还能 **往更长的文本上扩展**

> [RoPE](https://arxiv.org/abs/2104.09864)（2021）— 当今大模型的标配位置编码

----

## Mamba：能不能干脆不用注意力？

有没有一条完全不同的路？

- **状态空间模型 (SSM)** 处理文本只需一步一步来，简单又快速！
- 处理 10 万个字只需要 10 万次操作，而不是 100 亿次
- Mamba 还加入了 **选择机制** ，能自己决定记住什么、忘掉什么

> [Mamba](https://arxiv.org/abs/2312.00752)（2023）— 向 Transformer 的统治地位发起挑战

----

## Ring Attention：多块 GPU 接力跑

一张 GPU 的显存装不下超长文本怎么办？

- 把文本切成几段，分配给 **多块 GPU 组成的环**
- 每块 GPU 处理完自己的部分，再把键值数据传给下一块
- 就像接力赛——每人跑一棒，合力跑完全程

> [Ring Attention](https://arxiv.org/abs/2310.01889)（2023）— 靠多卡协作实现无限长上下文

----

## ModernBERT：老经典焕新生

BERT 在 2018 年惊艳了世界，但技术一直在进步。用现在的招数重建它会怎样？

- 位置编码换成 RoPE
- 注意力换成 FlashAttention
- 能读的文字量从 512 个词拉长到 8,192 个
- 结果：比原版又快又准

> [ModernBERT](https://arxiv.org/abs/2412.13663)（2024）— 经典架构 + 现代技巧 = 全面提升

----

## 第 5 章总结

| 论文 | 核心思想 |
|------|----------|
| [FlashAttention](https://arxiv.org/abs/2205.14135)（2022） | 优化内存访问 → 快 2-4 倍 |
| [GQA](https://arxiv.org/abs/2305.13245)（2023） | 多个注意力头共享缓存 |
| [RoPE](https://arxiv.org/abs/2104.09864)（2021） | 旋转位置编码，可扩展 |
| [Mamba](https://arxiv.org/abs/2312.00752)（2023） | 线性时间的注意力替代方案 |
| [Ring Attention](https://arxiv.org/abs/2310.01889)（2023） | 多卡协作处理超长文本 |
| [ModernBERT](https://arxiv.org/abs/2412.13663)（2024） | 经典 BERT + 现代技巧 |

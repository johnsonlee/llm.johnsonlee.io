<!-- .slide: class="center" -->

## 5. 高效注意力与架构改进

*每个字都仔细看虽好，可碰到长文本就太慢了！*

----

## 注意力机制的瓶颈

还记得吗？注意力机制让每个字去“扫一眼”所有其他字。这意味着：

- 10 个字 → 10 × 10 = **100** 次比较
- 1,000 个字 → 1,000 × 1,000 = **100 万** 次比较
- 100,000 个字 → 100,000 × 100,000 = **100 亿** 次比较！

文本越长，计算量就猛增——长度翻一倍，工作量翻四倍！有没有办法解决？

----

## FlashAttention：数学不变，换个搬法

诀窍不在于改公式，而在于改 **数据搬运的方式** 。

- GPU 里有“快但小”和“慢但大”两种内存
- 普通注意力要在两者之间来回搬数据，效率很低
- FlashAttention 重新编排了计算顺序，把数据搬运次数压到最少

计算结果完全一样，速度却 **快了 2-4 倍** ，显存占用也大幅降低！

> [FlashAttention](https://arxiv.org/abs/2205.14135)（2022）| [FlashAttention-2](https://arxiv.org/abs/2307.08691)（2023）

----

## 为什么内存速度这么重要

把 GPU 想象成一间厨房：

- **快内存 (SRAM)** = 灶台——很小，但东西伸手就够得到
- **慢内存 (HBM)** = 冰箱——空间大，但每次要走过去拿

普通注意力每用一种食材就跑一趟冰箱。FlashAttention 把菜谱重新编排：一次性从冰箱拿齐所有食材，全在灶台上处理完，最后一把放回去。同一道菜，少跑好多趟！

----

## GQA：大家一起用，别各存各的

标准注意力里，每个“注意力头”都有自己的一本笔记本来记住看过的内容。

32 个头就得存 32 份，太浪费了！

- **分组查询注意力 (GQA)** ：让好几个头 **共用** 一份缓存
- 32 个头只需 8 本笔记本 → 省了 4 倍的记忆空间
- 效果却几乎没有下降

> [GQA](https://arxiv.org/abs/2305.13245)（2023）— Llama 2、Mistral 等几乎所有主流大模型都在用

----

## GQA：前后对比

| | 标准多头注意力 | 分组查询注意力 (GQA) |
|---|---|---|
| 注意力头数 | 32 | 32 |
| Key-Value 副本数 | 32 | 8 |
| 每个 token 的缓存 | 100% | 约 25% |
| 质量 | 基准 | 几乎无差别 |

通过让一组注意力头共用 Key-Value 缓存，GQA 把显存省了 4 倍，效果却几乎没打折。这也是为什么几乎所有现代大语言模型——Llama 2、Mistral、Gemma——都在用它。

----

## RoPE：用“旋转”来表示位置

还记得前面说的位置编码——给每个字安排“座位号”吗？

最早的位置编码是固定的——训练时只见过 512 个字，所以只有 512 个“座位号”。一封短邮件（约 200 字）没问题，但《哈利·波特》 一章有约 5,000 字，几千个字根本没座位——大语言模型直接懵掉。

- RoPE 的做法是给词向量施加一个 **旋转变换** 来编码位置
- 临近的字旋转角度相近，远处的字旋转角度差异大
- 好处是训练完之后还能 **往更长的文本上扩展**

> [RoPE](https://arxiv.org/abs/2104.09864)（2021）— 当今大模型的标配位置编码

----

## Mamba：能不能干脆不用注意力？

有没有一条完全不同的路？

- **状态空间模型 (SSM)** 处理文本只需一步一步来，简单又快速！
- 处理 10 万个字只需要 10 万次操作，而不是 100 亿次
- Mamba 还加入了 **选择机制** ，能自己决定记住什么、忘掉什么

> [Mamba](https://arxiv.org/abs/2312.00752)（2023）— 向 Transformer 的统治地位发起挑战

----

## Transformer vs Mamba：各有所长

| | Transformer | Mamba (SSM) |
|---|---|---|
| 长文本速度 | 慢（二次方增长） | 快（线性增长） |
| 短文本质量 | 非常好 | 较好 |
| 显存占用 | 高 | 低 |
| 成熟度 | 久经考验 | 仍在验证中 |

最新的研究把两者结合：用 Transformer 层做复杂推理，用 Mamba 层做快速处理。一些混合模型已经做到了两全其美！

----

## Ring Attention：多块 GPU 接力跑

一张 GPU 的显存装不下超长文本怎么办？

- 把文本切成几段，分配给 **多块 GPU 组成的环**
- 每块 GPU 处理完自己的部分，再把键值数据传给下一块
- 就像接力赛——每人跑一棒，合力跑完全程

> [Ring Attention](https://arxiv.org/abs/2310.01889)（2023）— 靠多卡协作实现无限长上下文

----

## ModernBERT：老经典焕新生

BERT 在 2018 年惊艳了世界，但技术一直在进步。用现在的招数重建它会怎样？

- 位置编码换成 RoPE
- 注意力换成 FlashAttention
- 能读的文字量从 512 个词拉长到 8,192 个
- 结果：比原版又快又准

> [ModernBERT](https://arxiv.org/abs/2412.13663)（2024）— 经典架构 + 现代技巧 = 全面提升

----

## 第 5 章总结

| 论文 | 核心思想 |
|------|----------|
| [FlashAttention](https://arxiv.org/abs/2205.14135)（2022） | 优化内存访问 → 快 2-4 倍 |
| [GQA](https://arxiv.org/abs/2305.13245)（2023） | 多个注意力头共享缓存 |
| [RoPE](https://arxiv.org/abs/2104.09864)（2021） | 旋转位置编码，可扩展 |
| [Mamba](https://arxiv.org/abs/2312.00752)（2023） | 线性时间的注意力替代方案 |
| [Ring Attention](https://arxiv.org/abs/2310.01889)（2023） | 多卡协作处理超长文本 |
| [ModernBERT](https://arxiv.org/abs/2412.13663)（2024） | 经典 BERT + 现代技巧 |

----

## 想深入了解？

- [ELI5: FlashAttention (Aleksa Gordić)](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad) — 用图解简单讲解 FlashAttention
- [The Annotated Mamba (srush)](https://srush.github.io/annotated-mamba/hard.html) — 逐行拆解 Mamba 的代码
- [Rotary Position Embedding (blog)](https://blog.eleuther.ai/rotary-embeddings/) — 深入解析 RoPE 的原理
- [Visual Guide to Attention (Lilian Weng)](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/) — 全面梳理各种注意力变体

----

## 想一想

- FlashAttention 没有改数学公式，只是改了数据在内存里的搬运方式。你能想到其他“怎么做比做什么更重要”的例子吗？
- 注意力的计算量随长度平方增长：文本长 2 倍，工作量就变 4 倍。如果处理 1 万个词要 1 秒，处理 10 万个词要多久？
- Mamba 只能沿一个方向处理文本，而注意力机制可以看到所有位置。哪些任务对 Mamba 来说会比较吃力？
- ModernBERT 用 2024 年的技巧重建了 2018 年的大语言模型。如果你能用今天的技术重建一样旧东西，你会改造什么？

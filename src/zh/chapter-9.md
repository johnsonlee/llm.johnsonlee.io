<!-- .slide: class="center" -->

## 9. 检索增强生成

*要是 AI 能自己查资料就好了*

----

## 问题：AI 的知识是“定格”的

一个 2023 年训练好的模型，对 2024 年发生的事一无所知。

- “谁拿了 2024 年奥运金牌？” → 要么说不知道，要么一本正经地瞎编
- 大语言模型的知识在训练结束那一刻就 **冻住了**
- 每次更新知识都得重新训练，成本太高

> 如果大语言模型能在回答之前先去 **查一查** 呢？

----

## RAG：给 AI 发一张图书证

检索增强生成 (RAG) 的工作方式就像一个带着参考书的学生：

1. 收到问题：“法国的首都是哪里？”
2. 去知识库里 **检索** 相关段落
3. **阅读** 检索到的内容
4. 根据读到的东西 **组织回答**

大语言模型不需要把所有知识都背下来——会查就行。

> [RAG](https://arxiv.org/abs/2005.11401)（2020）— 先查资料再回答，有据可依

----

## 怎么搜索？靠嵌入向量 (Embeddings)

可是大语言模型怎么判断哪些段落跟问题相关？

- 把文字变成一串 **数字** （叫做向量），这样大语言模型就能比较两段话像不像
- 含义相近的文本 → 向量也靠得近
- “狗”和“小狗”的向量很近，“狗”和“代数”的向量就很远

要评估向量质量好不好，还得有统一的测试标准。

> [MTEB](https://arxiv.org/abs/2210.07316)（2022）— 给向量模型出一套统一考题

----

## ColBERT：更精细的匹配

普通检索是把整段文字压缩成一个向量，细节难免丢失。

ColBERT 换了个做法：

- 给段落里的 **每个词** 都保留一个独立向量
- 拿查询词逐个去跟段落词做对比
- 匹配更精准，速度依然够快

> [ColBERT](https://arxiv.org/abs/2004.12832)（2020）— 词级别的精细检索

----

## GraphRAG：把散落的线索串起来

有些问题的答案分散在 **多个文档** 里，怎么办？

- “A 公司和 B 公司是什么关系？”
- 普通 RAG 只检索单独的段落，可能漏掉关联信息
- GraphRAG 先构建一张 **知识图谱** ——把实体和关系都连起来
- 然后沿着图谱上的路径，把 A → C → B 之间的关系链找出来

> [GraphRAG](https://arxiv.org/abs/2404.16130)（2024）— 知识图谱 + RAG = 解决复杂关联问题

----

## 怎么验证 RAG 靠不靠谱？

RAG 系统也可能出岔子——检索到的内容不对、答案跑偏、凭空编造。

- **RAGAS** ：自动打分系统——检查搜到的内容对不对路、回答有没有瞎编、答案准不准
- **Self-RAG** ：让 AI 自己决定 **要不要去查** ——毕竟不是每个问题都需要翻书

> [RAGAS](https://arxiv.org/abs/2309.15217)（2023）| [Self-RAG](https://arxiv.org/abs/2310.11511)（2023）

----

## 绕不开的话题：幻觉

即使用了 RAG，模型有时候还是会编造内容：

- 引用根本不存在的文献
- 说出跟检索结果矛盾的话
- 把真实信息和虚构内容混在一起

搞清楚模型 **为什么会产生幻觉** ，是解决这个问题的第一步。

> [Hallucination Survey (Lilian Weng)](https://lilianweng.github.io/posts/2024-07-07-hallucination/)（2024）— 幻觉的成因与评估方法

----

## 第 9 章总结

| 论文 | 核心思想 |
|------|----------|
| [RAG](https://arxiv.org/abs/2005.11401)（2020） | 先查资料再回答 |
| [MTEB](https://arxiv.org/abs/2210.07316)（2022） | 文本嵌入的标准评测 |
| [ColBERT](https://arxiv.org/abs/2004.12832)（2020） | 词级别精细匹配 |
| [GraphRAG](https://arxiv.org/abs/2404.16130)（2024） | 知识图谱解决多跳问题 |
| [Self-RAG](https://arxiv.org/abs/2310.11511)（2023） | 模型自己决定要不要检索 |
| [Hallucination](https://lilianweng.github.io/posts/2024-07-07-hallucination/)（2024） | 为什么 AI 会“一本正经地胡说” |

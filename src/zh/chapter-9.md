<!-- .slide: class="center" -->

## 9. 检索增强生成

*要是 AI 能自己查资料就好了*

----

## 问题：AI 的知识是“定格”的

一个 2023 年训练好的大语言模型，对 2024 年发生的事一无所知。

- “谁拿了 2024 年奥运金牌？” → 要么说不知道，要么一本正经地瞎编
- 大语言模型的知识在训练结束那一刻就 **冻住了**
- 每次更新知识都得重新训练，成本太高

> 如果大语言模型能在回答之前先去 **查一查** 呢？

----

## RAG：给 AI 发一张图书证

检索增强生成 (RAG) 的工作方式就像一个带着参考书的学生：

1. 收到问题：“法国的首都是哪里？”
2. 去知识库里 **检索** 相关段落
3. **阅读** 检索到的内容
4. 根据读到的东西 **组织回答**

大语言模型不需要把所有知识都背下来——会查就行。

> [RAG](https://arxiv.org/abs/2005.11401)（2020）— 先查资料再回答，有据可依

----

## RAG vs 死记硬背：为什么重要

| | 没有 RAG | 有 RAG |
|---|---|---|
| 知识 | 只能靠训练时记住的 | 可以查阅你提供的任何文档 |
| 时效性 | 冻结在训练那天 | 随时可获取最新信息 |
| 来源 | “相信我” | “我从这里查到的” |
| 幻觉 | 常见 | 大幅减少 |

RAG 把大语言模型从“闭卷考试”变成了“开卷考试”——可以翻资料找答案，而不是全凭记忆猜。

----

## 怎么搜索？靠嵌入向量 (Embeddings)

可是大语言模型怎么判断哪些段落跟问题相关？

- 把文字变成一串 **数字** （叫做向量），这样大语言模型就能比较两段话像不像
- 含义相近的文本 → 向量也靠得近
- “狗”和“小狗”的向量很近，“狗”和“代数”的向量就很远

要评估向量质量好不好，还得有统一的测试标准。

> [MTEB](https://arxiv.org/abs/2210.07316)（2022）— 给向量模型出一套统一考题

----

## 嵌入向量：把文字变成坐标

想象每个词或句子都是 **地图上的一个点** ：

- “我喜欢狗” → 坐标 (2.1, 3.5, 0.8, ...)
- “我特别喜欢小狗” → 坐标 (2.0, 3.6, 0.9, ...) ——非常近！
- “股市暴跌了” → 坐标 (8.7, 1.2, 5.3, ...) ——很远

收到问题后，大语言模型把它也变成一个坐标点，然后找 **最近的** 文档点。点越近 = 内容越相关！

----

## ColBERT：更精细的匹配

普通检索是把整段文字压缩成一个向量，细节难免丢失。

ColBERT 换了个做法：

- 给段落里的 **每个词** 都保留一个独立向量
- 拿查询词逐个去跟段落词做对比
- 匹配更精准，速度依然够快

> [ColBERT](https://arxiv.org/abs/2004.12832)（2020）— 词级别的精细检索

----

## GraphRAG：把散落的线索串起来

有些问题的答案分散在 **多个文档** 里，怎么办？

- “A 公司和 B 公司是什么关系？”
- 普通 RAG 只检索单独的段落，可能漏掉关联信息
- GraphRAG 先构建一张 **知识图谱** ——把实体和关系都连起来
- 然后沿着图谱上的路径，把 A → C → B 之间的关系链找出来

> [GraphRAG](https://arxiv.org/abs/2404.16130)（2024）— 知识图谱 + RAG = 解决复杂关联问题

----

## 当简单的 RAG 不够用时

假设你问：“A 公司和 B 公司合并对股价有什么影响？”

- **简单 RAG** ：分别找到了关于 A 公司和 B 公司的段落——但没法把它们串联起来
- **GraphRAG** ：构建一张关系网：A 公司 → 合并了 → B 公司 → 股价 → 上涨 20%

当答案需要 **把分散在多个文档中的事实串起来** 时，GraphRAG 就能大显身手——这是简单搜索做不到的。

----

## 怎么验证 RAG 靠不靠谱？

RAG 系统也可能出岔子——检索到的内容不对、答案跑偏、凭空编造。

- **RAGAS** ：自动打分系统——检查搜到的内容对不对路、回答有没有瞎编、答案准不准
- **Self-RAG** ：让 AI 自己决定 **要不要去查** ——毕竟不是每个问题都需要翻书

> [RAGAS](https://arxiv.org/abs/2309.15217)（2023）| [Self-RAG](https://arxiv.org/abs/2310.11511)（2023）

----

## 绕不开的话题：幻觉

即使用了 RAG，大语言模型有时候还是会编造内容：

- 引用根本不存在的文献
- 说出跟检索结果矛盾的话
- 把真实信息和虚构内容混在一起

搞清楚大语言模型 **为什么会产生幻觉** ，是解决这个问题的第一步。

> [Hallucination Survey (Lilian Weng)](https://lilianweng.github.io/posts/2024-07-07-hallucination/)（2024）— 幻觉的成因与评估方法

----

## 第 9 章总结

| 论文 | 核心思想 |
|------|----------|
| [RAG](https://arxiv.org/abs/2005.11401)（2020） | 先查资料再回答 |
| [MTEB](https://arxiv.org/abs/2210.07316)（2022） | 文本嵌入的标准评测 |
| [ColBERT](https://arxiv.org/abs/2004.12832)（2020） | 词级别精细匹配 |
| [GraphRAG](https://arxiv.org/abs/2404.16130)（2024） | 知识图谱解决多跳问题 |
| [Self-RAG](https://arxiv.org/abs/2310.11511)（2023） | 大语言模型自己决定要不要检索 |
| [Hallucination](https://lilianweng.github.io/posts/2024-07-07-hallucination/)（2024） | 为什么 AI 会“一本正经地胡说” |

----

## 想深入了解？

- [What is RAG? (IBM)](https://research.ibm.com/blog/retrieval-augmented-generation-RAG) — 配图讲解 RAG 的入门指南
- [Embeddings Explained (Vicki Boykis)](https://vickiboykis.com/what_are_embeddings/) — 深入解析文字如何变成数字
- [GraphRAG Explained (Microsoft)](https://microsoft.github.io/graphrag/) — 微软出品的图检索入门教程
- [RAG From Scratch (LangChain)](https://www.youtube.com/watch?v=sVcwVQRHIc8) — 从零搭建一个 RAG 系统（视频）

----

## 想一想

- RAG 让大语言模型能“查资料”。但你怎么确定它查到的资料是可靠的？如果知识库里被人放了错误信息怎么办？
- 嵌入向量把“狗”和“小狗”放在很近的位置。你能想到哪些字面相似但含义完全不同的词吗？这会怎样误导搜索？
- GraphRAG 能把不同文档中的事实串联起来。你在学校的学习中，有没有需要综合多份资料才能回答的问题？
- 即使有了 RAG，大语言模型偶尔还是会“编造”内容。如果你要给医院做一个 AI 助手，你会怎么防止它胡说八道？

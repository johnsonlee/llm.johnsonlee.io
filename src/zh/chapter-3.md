<!-- .slide: class="center" -->

## 3. 缩放定律与训练理论

*脑子越大就一定越聪明吗？*

----

## 我们已经看到：大语言模型越大越聪明

GPT-2（15 亿）→ GPT-3（1750 亿），每一次“变大”都带来新能力。

但训练大模型动辄要花 **几百万美元** ，所以必须搞清楚一个问题：

> 预算有限的情况下，是该造一个 **更大的模型** ，还是准备 **更多的数据** ？

----

## 缩放定律 (Scaling Laws)：花钱的科学

研究人员发现了一条出奇简单的规律：

- 模型的表现可以用一个 **简单的数学规律** 来预测
- 参数越多 → 越好（但每多花一块钱，进步就小一点）
- 数据越多 → 越好（但每多花一块钱，进步就小一点）
- 算力越多 → 越好（但每多花一块钱，进步就小一点）

也就是说，你甚至可以在训练之前就 **估算** 出模型的表现！

> [Scaling Laws (Kaplan)](https://arxiv.org/abs/2001.08361)（2020）— AI 性能的预测公式

----

## 等等，之前的配方搞错了！

最初的结论是： **预算大头应该花在做大模型上** 。

但 DeepMind 团队发现正好相反：

- 大多数团队给模型喂的数据 **远远不够**
- 一个 700 亿参数的模型配上 4 倍数据，竟然打败了 2800 亿参数但数据少的模型
- 正确做法是让数据和参数 **同步增长**

> [Chinchilla](https://arxiv.org/abs/2203.15556)（2022）— “你们的模型都没喂饱！”

----

## 涌现能力 (Emergent Abilities)：突然开窍

大语言模型变大的过程中会出现很神奇的现象：

- 小模型做不了算术 → 稍大一点还是不行 → 再大一些—— **突然就会了！**
- 就好像突然“开窍”了一样，某些能力凭空冒了出来
- 这类能力被称为 **涌现能力** ，从来没有人专门教过它

> [Emergent Abilities](https://arxiv.org/abs/2206.07682)（2022）— 规模催生出的意外惊喜

----

## 真的是突然开窍吗？

也有人提出了不同意见：

- 也许这些能力并不是“突然”出现的
- 换一种 **评估方式** 去看，进步其实是 **循序渐进** 的
- 所谓的“魔法”，也许只是我们衡量方式造成的错觉

> [Are Emergent Abilities a Mirage?](https://arxiv.org/abs/2304.15004)（2023）— 泼一盆冷水也是好事

----

## 数据不够用了怎么办？

互联网虽大，数据量终究有限。当模型的胃口超过了现有的数据量，该怎么办？

- 可以 **重复使用** 数据，但超过约 4 轮之后效果就会下降
- 可以 **混入代码和数学** 等不同类型的数据来帮助学习
- 还可以 **生成合成数据** ——让 AI 给 AI 写训练材料

> [Scaling Data-Constrained Language Models](https://arxiv.org/abs/2305.16264)（2023）— 当数据成了瓶颈

----

## 第 3 章总结

| 论文 | 核心思想 |
|------|----------|
| [Scaling Laws](https://arxiv.org/abs/2001.08361)（2020） | 模型表现有规律可循，花钱之前就能估算 |
| [Chinchilla](https://arxiv.org/abs/2203.15556)（2022） | 数据要跟上——大部分模型都没喂饱 |
| [Emergent Abilities](https://arxiv.org/abs/2206.07682)（2022） | 模型够大之后，新能力突然涌现 |
| [Mirage?](https://arxiv.org/abs/2304.15004)（2023） | 也许是评估方式在搞鬼 |
| [Data-Constrained](https://arxiv.org/abs/2305.16264)（2023） | 数据有限时的应对策略 |

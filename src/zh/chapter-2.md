<!-- .slide: class="center" -->

## 2. GPT 家族

*如果给大语言模型喂更多数据、加更多“脑细胞”，会发生什么？*

----

## 会预测下一个词的大语言模型……

前面我们知道了 GPT-1 是靠“猜下一个词”来学语言的。

不过 GPT-1 体量很小，每做一个新任务都得单独教它。

> 那如果把大语言模型做得 **大得多** 呢？它能不能自己就学会做各种任务？

----

## GPT-2：不教就会！

研究人员把参数量拉到了 **15 亿** ，结果发生了意想不到的事：

- 没有针对任何特定任务做训练，GPT-2 居然就能翻译、写摘要、回答问题
- 只要给它一个好的 **提示词** ，不需要任何示例就能动手干活

这种“没学过就能做”的能力叫做 **零样本 (Zero-Shot)** 学习。

> [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)（2019）— 语言模型天生就是多面手

----

## 零样本学习是怎么做到的？

想象一个学生读遍了图书馆的所有书，但从来没有上过数学课：

- 老师：“12 + 7 等于多少？”
- 学生：“嗯，我在好多书里见过算术题……答案是 19！”

GPT-2 从来没被 *教过* 翻译，但它读过大量的翻译文本，所以它已经“会了”。知识早就在那里——只需要一个合适的问题来激活它。

----

## GPT-3：看几个例子就够了

如果继续做大——大到 **1750 亿** 参数呢？

- 在提示词里给它 **几个例子** ，它就能学会新任务
- “英译法：sea otter → loutre de mer, cheese →” → “fromage”
- 不用重新训练，给个模式它就能举一反三

这叫 **少样本学习 (Few-Shot Learning)** ，也叫 **上下文学习 (In-Context Learning)** 。

> [GPT-3](https://arxiv.org/abs/2005.14165)（2020）— 用规模证明：大力真能出奇迹

----

## 上下文学习：现学现用

少样本学习的神奇之处在于——大语言模型不需要重新训练，它直接从 **提示词里的例子** 中学习：

- “cat → gato, dog → perro, house → ?”
- 大语言模型发现了规律，回答：“casa”

就像你教朋友打牌：看了两三轮，他就“懂了”，可以自己上手。大语言模型也一样——只需要几个例子，就能举一反三。

----

## 能不能写代码？

GPT-3 从文本中学会了语言，那如果拿 **代码** 来训练呢？

- Codex 在数百万个 GitHub 仓库上做了训练
- 你用一句话描述需求，它就能写出可运行的程序
- “写一个 Python 函数来排序列表” → 直接给你代码！

它后来成了 GitHub Copilot 背后的引擎。

> [Codex](https://arxiv.org/abs/2107.03374)（2021）— 把自然语言变成可运行的程序

----

## 但是，问题来了

GPT-3 虽然强大，却有不少毛病：

- 有时候一本正经地胡说八道
- 偶尔会输出不友好甚至有害的内容
- 它只是在“预测最可能的下一个词”，并不是在 **理解你的意图**

> 怎么才能让它真正 **听话** 、真正 **有用** 呢？

----

## InstructGPT：跟人类学规矩

办法是让 **人类来当老师** ，给大语言模型的回答打分！

1. 先由人类撰写一批优质回答作为示范
2. 大语言模型学习这些示范
3. 让大语言模型同时给出多个答案，人类给这些答案 **排出好坏**
4. 大语言模型学着去讨好人类的偏好

这套方法叫 **RLHF** （基于人类反馈的强化学习），正是后来 ChatGPT 的基石。

> [InstructGPT](https://arxiv.org/abs/2203.02155)（2022）— ChatGPT 的技术基础

----

## 为什么 RLHF 是个里程碑？

没有 RLHF 之前，和大语言模型对话其实很让人抓狂：

- 你：“写一首关于小狗的短诗”
- 旧大语言模型：“犬，哺乳纲食肉目犬科，特征为……”（它只是在预测最可能的文本！）
- 经过 RLHF 之后：“毛茸茸的小家伙，摇着尾巴满地跑，清晨小路上留下一串串快乐的脚印……”

RLHF 让大语言模型学会了 *预测文本* 和 *真正帮忙* 之间的区别。就是这一个想法，把文本预测器变成了 ChatGPT。

----

## GPT-4：不只会读文字了

要是大语言模型还能 **看懂图片** 呢？

- GPT-4 能理解照片、图表、示意图
- 推理和复杂任务上的表现大幅提升
- 安全性也显著增强，更不容易产生有害内容

> [GPT-4](https://arxiv.org/abs/2303.08774)（2023）— 文字+视觉，迈入多模态时代

----

## o1：想好了再回答

做难题的时候，你是不是也得 **先在脑子里想清楚** 再动笔？

- 之前的大语言模型想都不想就给答案，经常出错
- o1 会在回答之前多花一些 **思考时间** ，一步一步推演
- 就像考试时先打草稿再写答案，而不是直接猜

> [o1/o3: Deliberative Alignment](https://openai.com/index/deliberative-alignment/)（2024）— 先想后答的推理模型

----

## “思考时间”有什么用？

对比两种解难题的方式：

- **旧方式** ：“答案是 42”（秒答，经常错）
- **o1 方式** ：“让我想想……先把问题拆成几个部分……第一步得出 X……第二步得出 Y……合在一起……答案是 37”

多出来的思考时间，让大语言模型能发现自己的错误、尝试不同思路、还能自我检验——就像你考试时先打草稿再写最终答案一样。

----

## 第 2 章总结

| 论文 | 核心思想 |
|------|----------|
| [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)（2019） | 大语言模型做大 → 不教就会 |
| [GPT-3](https://arxiv.org/abs/2005.14165)（2020） | 1750 亿参数 → 看几个例子就能学 |
| [Codex](https://arxiv.org/abs/2107.03374)（2021） | 用代码训练 → 能写程序 |
| [InstructGPT](https://arxiv.org/abs/2203.02155)（2022） | 人类打分 → 学会听指令 (RLHF) |
| [GPT-4](https://arxiv.org/abs/2303.08774)（2023） | 多模态 — 文字 + 图片都懂 |
| [o1/o3](https://openai.com/index/deliberative-alignment/)（2024） | 想好了再回答 |

----

## 想深入了解？

- [Jay Alammar: The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) — 用图解展示 GPT-2 如何生成文本
- [Stephen Wolfram: What Is ChatGPT Doing?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/) — 深入浅出地讲解 ChatGPT 的原理
- [Andrej Karpathy: Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g) — 一小时纵览 GPT 的进化之路
- [RLHF Explained Simply](https://huggingface.co/blog/rlhf) — 人类反馈如何塑造 AI 行为

----

## 想一想

- GPT-2 有 15 亿参数，GPT-3 有 1750 亿。如果只能再多给 100 亿，你会把它们用在“理解”还是“生成”上？为什么？
- InstructGPT 学的是人类的偏好。但不同的人喜好不同——遇到分歧时，大语言模型该怎么办？
- o1 会“想好了再回答”。你能想到什么情况下，想太久反而不如快速作答？
- 如果你能教大语言模型一个目前还不具备的技能，你会选什么？

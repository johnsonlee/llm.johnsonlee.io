<!-- .slide: class="center" -->

## 12. 多模态模型

*AI 能不能不只看文字，还能看懂图片？*

----

## 纯文本 AI 是个“睁眼瞎”

GPT 和 BERT 处理文字很厉害，但它们：

- 看不懂一张照片里有什么
- 读不了文档截图上的文字
- 理解不了图表和示意图

> 人靠五官感知世界，AI 能不能也这样？

----

## CLIP：让图片和文字“对上号”

核心思路：互联网上有无数张带文字说明的图片，让 AI 从中学习！

- 喂给模型几百万张带文字说明的图片
- 它学会了“一张狗的照片”和“狗”这个词应该对应
- 学会之后，给它任何一张新图片、任何一个分类标签，它都能自动匹配—— **不需要专门训练**

> [CLIP](https://arxiv.org/abs/2103.00020)（2021）— 用自然语言实现零样本图像识别

----

## LLaVA：让聊天机器人学会“看”

CLIP 能把图片和文字配对，但做不到 **围绕图片聊天** 。

LLaVA 的做法：

- 拿一个语言模型（比如 Llama）当“大脑”
- 再接上一个视觉编码器（比如 CLIP）当“眼睛”
- 用图文对话数据做微调：“这张图里有什么？” → “马路上有一辆红色轿车”

> [LLaVA](https://arxiv.org/abs/2304.08485)（2023）— 开源的多模态聊天机器人

----

## Gemini：天生就是多模态的

LLaVA 是在文字模型上“外挂”了视觉。那如果从一开始就为多种模态而设计呢？

- **天生就是多模态** ：从一开始就把文字、图片、声音、视频放在一起训练
- 能看懂图表、读手写体、分析视频内容
- 多模态不是后加的补丁，而是写进了它的 DNA

> [Gemini](https://arxiv.org/abs/2312.11805)（2023）— Google 的原生多模态大模型

----

## ColPali：直接“看着”文档搜索

传统的文档搜索得先把图片里的文字提取出来：

1. 用文字识别软件 (OCR) 把图里的字提取出来 → 2. 建个可搜索的目录 → 3. 再搜索

ColPali 把这些步骤全省了：

- 直接把 **整页截图** 丢给视觉模型
- 不需要 OCR，模型“看”一眼页面就能理解
- 对表格、图表、复杂排版都适用

> [ColPali](https://arxiv.org/abs/2407.01449)（2024）— 免 OCR 的文档检索

----

## 更多视觉模型

- **Pixtral** （Mistral）：任意分辨率的图片都能处理，不用缩放裁切
- **Molmo** （AI2）：不光能描述图片内容，还能精确地 **指出** 物体在图中的位置

> [Pixtral](https://arxiv.org/abs/2410.07073)（2024）| [Molmo](https://arxiv.org/abs/2409.17146)（2024）

----

## 第 12 章总结

| 论文 | 核心思想 |
|------|----------|
| [CLIP](https://arxiv.org/abs/2103.00020)（2021） | 图文配对 → 零样本图像理解 |
| [LLaVA](https://arxiv.org/abs/2304.08485)（2023） | 给语言模型装上“眼睛” |
| [Gemini](https://arxiv.org/abs/2312.11805)（2023） | 天生多模态，从头一起训 |
| [ColPali](https://arxiv.org/abs/2407.01449)（2024） | 看图搜文档，免 OCR |
| [Pixtral](https://arxiv.org/abs/2410.07073) / [Molmo](https://arxiv.org/abs/2409.17146)（2024） | 灵活分辨率 / 精准定位 |

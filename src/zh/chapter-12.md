<!-- .slide: class="center" -->

## 12. 多模态模型

*AI 能不能不只看文字，还能看懂图片？*

----

## 纯文本 AI 是个“睁眼瞎”

GPT 和 BERT 处理文字很厉害，但它们：

- 看不懂一张照片里有什么
- 读不了文档截图上的文字
- 理解不了图表和示意图

> 人靠五官感知世界，AI 能不能也这样？

----

## CLIP：让图片和文字“对上号”

核心思路：互联网上有无数张带文字说明的图片，让 AI 从中学习！

- 喂给大语言模型几百万张带文字说明的图片
- 它学会了“一张狗的照片”和“狗”这个词应该对应
- 学会之后，给它任何一张新图片、任何一个分类标签，它都能自动匹配—— **不需要专门训练**

> [CLIP](https://arxiv.org/abs/2103.00020)（2021）— 用自然语言实现零样本图像识别

----

## CLIP 怎么学的：一场配对游戏

想象一个有几百万张卡片的游戏：

1. 每张卡片一面是 **图片** ，另一面是 **文字描述**
2. 把所有图片和描述分开打乱
3. 大语言模型必须把每张图片和正确的描述配对
4. 练得多了，它就学会了“狗”长什么样、“日落”长什么样……

神奇之处在于：训练结束后，你可以说“帮我找一张金毛犬在雪地里玩耍的照片”——CLIP 真的能找到，即使它从没见过这个确切的描述！

----

## LLaVA：让聊天机器人学会“看”

CLIP 能把图片和文字配对，但做不到 **围绕图片聊天** 。

LLaVA 的做法：

- 拿一个语言模型（比如 Llama）当“大脑”
- 再接上一个视觉编码器（比如 CLIP）当“眼睛”
- 用图文对话数据做微调：“这张图里有什么？” → “马路上有一辆红色轿车”

> [LLaVA](https://arxiv.org/abs/2304.08485)（2023）— 开源的多模态聊天机器人

----

## LLaVA 怎么把“眼睛”和“大脑”连起来

LLaVA 的架构出奇地简单：

1. **眼睛** （视觉编码器）：CLIP 看图片，把看到的内容变成一串数字
2. **翻译桥梁** （投影层）：一个小型转换模块把图片数字“翻译”成语言模型能懂的格式
3. **大脑** （语言模型）：Llama 接收翻译后的图片信息 + 你的问题，生成回答

就像一个懂“图片语”的朋友帮你翻译给一个只懂中文的朋友听！

----

## Gemini：天生就是多模态的

LLaVA 是在文字大语言模型上“外挂”了视觉。那如果从一开始就为多种模态而设计呢？

- **天生就是多模态** ：从一开始就把文字、图片、声音、视频放在一起训练
- 能看懂图表、读手写体、分析视频内容
- 多模态不是后加的补丁，而是写进了它的 DNA

> [Gemini](https://arxiv.org/abs/2312.11805)（2023）— Google 的原生多模态大模型

----

## “外挂式” vs “原生”多模态

| | 外挂式（LLaVA） | 原生多模态（Gemini） |
|---|---|---|
| 做法 | 在现有文本大语言模型上加视觉模块 | 从第一天起就同时训练所有模态 |
| 优势 | 更省钱，可以复用现有大语言模型 | 跨模态理解更深入 |
| 局限 | 图片和文字像“两个脑子” | 需要从零开始大规模训练 |
| 类比 | 成年后学第二门语言 | 从小在双语环境中长大 |

两种路线各有千秋——外挂式上手快，但原生多模态对图文关系的理解通常更自然。

----

## ColPali：直接“看着”文档搜索

传统的文档搜索得先把图片里的文字提取出来：

1. 用文字识别软件 (OCR) 把图里的字提取出来 → 2. 建个可搜索的目录 → 3. 再搜索

ColPali 把这些步骤全省了：

- 直接把 **整页截图** 丢给视觉模型
- 不需要 OCR，大语言模型“看”一眼页面就能理解
- 对表格、图表、复杂排版都适用

> [ColPali](https://arxiv.org/abs/2407.01449)（2024）— 免 OCR 的文档检索

----

## 更多视觉模型

- **Pixtral** （Mistral）：任意分辨率的图片都能处理，不用缩放裁切
- **Molmo** （AI2）：不光能描述图片内容，还能精确地 **指出** 物体在图中的位置

> [Pixtral](https://arxiv.org/abs/2410.07073)（2024）| [Molmo](https://arxiv.org/abs/2409.17146)（2024）

----

## 第 12 章总结

| 论文 | 核心思想 |
|------|----------|
| [CLIP](https://arxiv.org/abs/2103.00020)（2021） | 图文配对 → 零样本图像理解 |
| [LLaVA](https://arxiv.org/abs/2304.08485)（2023） | 给语言模型装上“眼睛” |
| [Gemini](https://arxiv.org/abs/2312.11805)（2023） | 天生多模态，从头一起训 |
| [ColPali](https://arxiv.org/abs/2407.01449)（2024） | 看图搜文档，免 OCR |
| [Pixtral](https://arxiv.org/abs/2410.07073) / [Molmo](https://arxiv.org/abs/2409.17146)（2024） | 灵活分辨率 / 精准定位 |

----

## 想深入了解？

- [The Illustrated Stable Diffusion (Jay Alammar)](https://jalammar.github.io/illustrated-stable-diffusion/) — 图解扩散模型如何用 CLIP 实现文生图
- [LLaVA Demo](https://llava-vl.github.io/) — 亲自试试跟 AI 聊图片
- [Gemini Technical Report](https://deepmind.google/technologies/gemini/) — Google 对多模态 AI 的深度解析
- [Multimodal AI Explained (IBM)](https://www.ibm.com/think/topics/multimodal-ai) — 多模态模型的清晰入门

----

## 想一想

- CLIP 靠图文配对来学习。如果网上的图片描述有误或有偏见，会对大语言模型产生什么影响？
- LLaVA 在文本大语言模型上“外挂”视觉，像成年后学外语。Gemini 天生就是多模态，像从小在双语环境长大。你觉得长远来看哪种路线会胜出？
- ColPali 通过“看”文档来搜索，而不是提取文字。这对哪类文档最有效？哪些场景它可能搞不定？
- 如果大语言模型能像理解文字一样理解视频，会催生哪些全新的应用？

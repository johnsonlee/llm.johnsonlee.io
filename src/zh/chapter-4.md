<!-- .slide: class="center" -->

## 4. 开源大模型里程碑

*假如强大的 AI 人人都能免费用呢？*

----

## 封闭 AI 的困境

GPT-3 和 GPT-4 确实厉害，可是：

- 只有 OpenAI 自己能用
- 别人得 **按次付费** ，还看不到它内部怎么运作
- 没法修改，也没法在自己的机器上跑

> 要是有人把强大的大语言模型 **开源免费** 放出来呢？

----

## LLaMA：推开大门

Meta 发布了 LLaMA——从 70 亿到 650 亿参数不等，全部用 **公开数据** 训练。

- 体量虽小，跟 GPT-3 掰手腕竟然不落下风
- 谁都可以下载、研究、拿去做实验
- 直接引爆了开源大模型的研发热潮

> [LLaMA](https://arxiv.org/abs/2302.13971)（2023）— 点燃开源 LLM 革命的火种

----

## 开源为什么重要

在 LLaMA 之前，研究大语言模型就像面对一个上了锁的百宝箱：

- 你可以通过 API **使用** GPT-3，但看不到里面怎么运作
- 出了错你也 **找不出原因**
- 没法按自己的需求 **定制**

LLaMA 把钥匙交到了所有人手里。几周之内，就有人造出了医疗助手、编程助手、语言老师——全部基于这一个开源模型。

----

## Llama 2 & 3：越来越强、越来越安全

Meta 后续持续迭代：

- **Llama 2** ：加入了 RLHF 安全训练，把所有技术细节都公开了
- **Llama 3** ：参数量扩大到 4050 亿，附带详尽的训练文档

每一代都更强、更透明、更易获取。

> [Llama 2](https://arxiv.org/abs/2307.09288)（2023）| [Llama 3](https://arxiv.org/abs/2407.21783)（2024）

----

## 小模型也能很能打

一定要做大才行吗？Mistral 给出了不同答案：

- 只有 70 亿参数，单张 GPU 就能跑
- 用上了 **滑动窗口注意力** ——只看附近的词就够了，省时又省力
- 还用了 **分组查询注意力** ——让大脑的各个部分共享笔记，不用每人抄一份

> [Mistral 7B](https://arxiv.org/abs/2310.06825)（2023）— 以小博大的典范

----

## 专家混合 (Mixture of Experts)：术业有专攻

做数学题的时候，你用不着动用大脑里管音乐的那部分。AI 能不能也这样？

- **Mixtral** 里有 8 个“专家”网络，每个 70 亿参数
- 来了任务，只派 **2 个专家** 上场，其余 6 个不动
- 总参数 470 亿，但每次实际运算只用 130 亿！

> [Mixtral](https://arxiv.org/abs/2401.04088)（2024）— 让对的专家做对的事

----

## 专家混合到底怎么运作的？

想象一家有多位专科医生的医院：

1. 病人来了 → 前台（叫 **路由器** ）看一下症状
2. 路由器把病人分给对的专科医生——比如心脏科和呼吸科
3. 只有这 2 位医生处理这个病例，其余 6 位休息
4. 下一个病人可能需要完全不同的专家

这就是 MoE 的原理！路由器决定 8 个专家中由哪 2 个来处理每段文本。总知识量：470 亿参数。每次实际运算：只用 130 亿。

----

## DeepSeek：把效率做到极致

来自中国的 DeepSeek 在效率上不断刷新纪录：

- **DeepSeek-V2** ：发明了一种新方法来压缩 AI 的记忆空间，省下大量显存
- **DeepSeek-V3** ：总参数 6710 亿，但用了一种省钱的数字存储方式，训练成本砍了一半
- **DeepSeek-R1** ：纯靠强化学习训练，大语言模型 **自己学会了推理**

> [DeepSeek-V2](https://arxiv.org/abs/2405.04434) | [V3](https://arxiv.org/abs/2412.19437) | [R1](https://arxiv.org/abs/2501.12948)（2024-2025）

----

## DeepSeek-R1：自己学会了推理

大多数大语言模型是从人类写好的推理示例中学习的。DeepSeek-R1 走了一条不同的路：

- 从一个 **没有任何推理训练** 的基础模型开始
- 给它数学题，答对了就给奖励
- 大语言模型通过反复试错 **自己发明了** 推理策略
- 它自发学会了“让我检查一下”“让我换个思路”——完全没人教过

这证明了推理能力可以从纯强化学习中 **自然涌现** 。

----

## 全球各地的开源力量

- **Qwen3** （阿里巴巴）：支持 100 多种语言，思考/直答两种模式无缝切换
- **Gemma 2** （Google）：大模型“教”小模型——就像学霸哥哥辅导弟弟写作业

> 全球的开源力量越来越强，遍地开花！

> [Qwen3](https://arxiv.org/abs/2505.09388)（2025）| [Gemma 2](https://arxiv.org/abs/2408.00118)（2024）

----

## 第 4 章总结

| 论文 | 核心思想 |
|------|----------|
| [LLaMA](https://arxiv.org/abs/2302.13971)（2023） | 掀起开源 LLM 革命 |
| [Llama 2](https://arxiv.org/abs/2307.09288) / [3](https://arxiv.org/abs/2407.21783) | 开源 + 安全对齐 |
| [Mistral 7B](https://arxiv.org/abs/2310.06825)（2023） | 小模型，大能量 |
| [Mixtral](https://arxiv.org/abs/2401.04088)（2024） | 专家混合——只激活需要的部分 |
| [DeepSeek 系列](https://arxiv.org/abs/2501.12948)（2024-25） | 极致效率，自学推理 |
| [Qwen3](https://arxiv.org/abs/2505.09388) / [Gemma 2](https://arxiv.org/abs/2408.00118) | 全球开源百花齐放 |

----

## 想深入了解？

- [The LLaMA Story (Weights & Biases)](https://wandb.ai/ayush-thakur/llama-2/reports/All-About-Llama-2--Vmlldzo0OTk0NjQ2) — LLaMA 如何改变了 AI 格局
- [Mixture of Experts Explained (Hugging Face)](https://huggingface.co/blog/moe) — 图解 MoE 的工作原理
- [DeepSeek-R1 Explained (AI Explained)](https://www.youtube.com/watch?v=bAWV_yrqx1A) — 一个大语言模型是如何自学推理的
- [Open Source AI Timeline](https://lifearchitect.ai/models-table/) — 追踪所有开源模型的发布动态

----

## 想一想

- Meta 花了几百万美元训练的大语言模型为什么要免费送人？开源对他们有什么好处？
- Mixtral 有 8 个专家但每次只用 2 个。如果路由器总是选同样的 2 个专家，会出什么问题？
- DeepSeek-R1 通过纯强化学习自学了推理。你觉得大语言模型能不能用同样的方式自学其他能力——比如创造力或幽默感？
- 如果你可以免费下载并定制任何一个大语言模型，你会用它来做什么？

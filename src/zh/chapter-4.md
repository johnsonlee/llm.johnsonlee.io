<!-- .slide: class="center" -->

## 4. 开源大模型里程碑

*假如强大的 AI 人人都能免费用呢？*

----

## 封闭 AI 的困境

GPT-3 和 GPT-4 确实厉害，可是：

- 只有 OpenAI 自己能用
- 别人得 **按次付费** ，还看不到它内部怎么运作
- 没法修改，也没法在自己的机器上跑

> 要是有人把强大的模型 **开源免费** 放出来呢？

----

## LLaMA：推开大门

Meta 发布了 LLaMA——从 70 亿到 650 亿参数不等，全部用 **公开数据** 训练。

- 体量虽小，跟 GPT-3 掰手腕竟然不落下风
- 谁都可以下载、研究、拿去做实验
- 直接引爆了开源大模型的研发热潮

> [LLaMA](https://arxiv.org/abs/2302.13971)（2023）— 点燃开源 LLM 革命的火种

----

## Llama 2 & 3：越来越强、越来越安全

Meta 后续持续迭代：

- **Llama 2** ：加入了 RLHF 安全训练，把所有技术细节都公开了
- **Llama 3** ：参数量扩大到 4050 亿，附带详尽的训练文档

每一代都更强、更透明、更易获取。

> [Llama 2](https://arxiv.org/abs/2307.09288)（2023）| [Llama 3](https://arxiv.org/abs/2407.21783)（2024）

----

## 小模型也能很能打

一定要做大才行吗？Mistral 给出了不同答案：

- 只有 70 亿参数，单张 GPU 就能跑
- 用上了 **滑动窗口注意力** ——只看附近的词就够了，省时又省力
- 还用了 **分组查询注意力** ——让大脑的各个部分共享笔记，不用每人抄一份

> [Mistral 7B](https://arxiv.org/abs/2310.06825)（2023）— 以小博大的典范

----

## 专家混合 (Mixture of Experts)：术业有专攻

做数学题的时候，你用不着动用大脑里管音乐的那部分。AI 能不能也这样？

- **Mixtral** 里有 8 个“专家”网络，每个 70 亿参数
- 来了任务，只派 **2 个专家** 上场，其余 6 个不动
- 总参数 470 亿，但每次实际运算只用 130 亿！

> [Mixtral](https://arxiv.org/abs/2401.04088)（2024）— 让对的专家做对的事

----

## DeepSeek：把效率做到极致

来自中国的 DeepSeek 在效率上不断刷新纪录：

- **DeepSeek-V2** ：发明了一种新方法来压缩 AI 的记忆空间，省下大量显存
- **DeepSeek-V3** ：总参数 6710 亿，但用了一种省钱的数字存储方式，训练成本砍了一半
- **DeepSeek-R1** ：纯靠强化学习训练，模型 **自己学会了推理**

> [DeepSeek-V2](https://arxiv.org/abs/2405.04434) | [V3](https://arxiv.org/abs/2412.19437) | [R1](https://arxiv.org/abs/2501.12948)（2024-2025）

----

## 全球各地的开源力量

- **Qwen3** （阿里巴巴）：支持 100 多种语言，思考/直答两种模式无缝切换
- **Gemma 2** （Google）：大模型“教”小模型——就像学霸哥哥辅导弟弟写作业

> 全球的开源力量越来越强，遍地开花！

> [Qwen3](https://arxiv.org/abs/2505.09388)（2025）| [Gemma 2](https://arxiv.org/abs/2408.00118)（2024）

----

## 第 4 章总结

| 论文 | 核心思想 |
|------|----------|
| [LLaMA](https://arxiv.org/abs/2302.13971)（2023） | 掀起开源 LLM 革命 |
| [Llama 2](https://arxiv.org/abs/2307.09288) / [3](https://arxiv.org/abs/2407.21783) | 开源 + 安全对齐 |
| [Mistral 7B](https://arxiv.org/abs/2310.06825)（2023） | 小模型，大能量 |
| [Mixtral](https://arxiv.org/abs/2401.04088)（2024） | 专家混合——只激活需要的部分 |
| [DeepSeek 系列](https://arxiv.org/abs/2501.12948)（2024-25） | 极致效率，自学推理 |
| [Qwen3](https://arxiv.org/abs/2505.09388) / [Gemma 2](https://arxiv.org/abs/2408.00118) | 全球开源百花齐放 |

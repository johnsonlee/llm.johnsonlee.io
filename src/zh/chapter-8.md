<!-- .slide: class="center" -->

## 8. 参数高效微调

*能不能不从头来，直接教 AI 学新本事？*

----

## 难题：给一个巨人换脑子

你想让 AI 变成一个医疗助手，但基础模型只是个“什么都知道一点”的通才，医学知识远远不够。

- 重新训练全部 700 亿参数得花 **几百万**
- 需要几十张昂贵的 GPU
- 一轮训练动辄好几周

> 能不能不动模型大脑、只加点“医学笔记”就好？

----

## LoRA：给大脑贴便利贴

想象你的大脑是一块冻住的巨大冰块，你不需要把整块冰化了重新冻：

- 在模型的每一层旁边挂一个 **小小的可训练模块**
- 只训练这些小模块，原始模型完全不动
- 这些小模块加起来只有原模型 **0.1%** 的参数量！

就像在课本上贴便利贴，而不是把课本重写一遍。

> [LoRA](https://arxiv.org/abs/2106.09685)（2021）— 如今微调的行业标准

----

## QLoRA：一张消费级显卡就能微调

LoRA 训练很省，但光是把 650 亿参数的模型装进内存就得用好几张昂贵的显卡。

QLoRA 叠了两个 buff：

- 先把冻结的模型 **量化压缩** 到 4 比特（体积缩小 16 倍！）
- 然后在压缩后的模型上照常用 LoRA 微调
- 一张 **消费级 GPU** 就能搞定 650 亿参数模型的微调

> [QLoRA](https://arxiv.org/abs/2305.14314)（2023）— 4 比特量化 + LoRA = 人人都能微调

----

## DoRA：LoRA 的升级版

研究人员深入分析了微调到底在调什么，发现：

- 权重的变化可以拆成两部分： **方向** （往哪儿调）和 **幅度** （调多少）
- 普通 LoRA 把两者混在一起调
- DoRA 把它们拆开，分别独立调整

结果是：花一样的钱，效果比 LoRA 更好。

> [DoRA](https://arxiv.org/abs/2402.09353)（2024）— 先拆分、再适配

----

## Prefix-Tuning：只训练“开场白”

换个思路：不碰模型本身，只是在输入前面加几个 **可学习的虚拟 token** 。

- 模型本身完全不动
- 只训练这些“开场白”魔法词
- 它们会学着把模型的输出引向你想要的方向

> [Prefix-Tuning](https://arxiv.org/abs/2101.00190)（2021）— 只训练前缀，不动模型

----

## 实用工具和指南

微调方法这么多，该怎么选？

- **PEFT 综述** ：系统梳理各种方法的优劣和适用场景
- **LLaMA-Factory** ：一站式微调工具，支持 LoRA、QLoRA 等主流方法，几次点击就能搞定 100 多种模型的微调

> [PEFT Survey](https://arxiv.org/abs/2303.15647)（2022）| [LLaMA-Factory](https://arxiv.org/abs/2403.13372)（2024）

----

## 第 8 章总结

| 论文 | 核心思想 |
|------|----------|
| [LoRA](https://arxiv.org/abs/2106.09685)（2021） | 小模块挂上去，只占 0.1% 参数 |
| [QLoRA](https://arxiv.org/abs/2305.14314)（2023） | 4 比特压缩 + LoRA = 一张卡搞定 |
| [DoRA](https://arxiv.org/abs/2402.09353)（2024） | 拆分方向和幅度，分别适配 |
| [Prefix-Tuning](https://arxiv.org/abs/2101.00190)（2021） | 用可学习的前缀引导模型 |
| [LLaMA-Factory](https://arxiv.org/abs/2403.13372)（2024） | 一站式微调工具 |

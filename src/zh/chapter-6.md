<!-- .slide: class="center" -->

## 6. 对齐与安全

*怎么让 AI 既有用又无害？*

----

## 问题：AI 只是在“学舌”

一个靠预测下一个词训练出来的大语言模型，会输出 **概率最高的话** ——这意味着它也可能说出：

- 带偏见、令人不适甚至有害的内容
- 听起来很自信的假话（幻觉）
- 完全跑偏、答非所问的回答

> 怎样才能让它既 **有帮助** ，又不 **闯祸** ？

----

## RLHF：请人类来当老师

思路：就像教孩子一样，用 **人类的反馈** 来引导大语言模型！

1. 让大语言模型同时生成好几个回答
2. 人类把这些回答 **排个名次**
3. 用排名数据训练一个“奖励模型”，学会分辨好坏
4. 再训练语言模型去追求更高的奖励

> [Training a Helpful and Harmless Assistant](https://arxiv.org/abs/2204.05862)（2022）— Anthropic 提出的 RLHF 方法

----

## RLHF 流程详解

1. **监督微调 (SFT)** ：先让大语言模型看一批人类写的优质回答，跟着学
2. **训练奖励模型** ：人类对成对的回答排名（“A 比 B 好”），训练一个专门的大语言模型学会像人类一样打分
3. **强化学习优化** ：大语言模型生成回答，奖励模型打分，大语言模型调整自己以获得更高分

就像训练小狗：先示范动作（SFT），然后做对了给零食（RL），小狗就学会了重复那些能拿到奖励的行为！

----

## Constitutional AI：让 AI 自己管自己

收集人类反馈 **又贵又慢** 。能不能让 AI 自己给自己打分？

- 给 AI 定一套 **行为准则** （相当于一部“宪法”）
- “要有帮助、要诚实、不能伤害人”
- AI 根据这些准则审视并修改自己的回答

> [Constitutional AI](https://arxiv.org/abs/2212.08073)（2022）— 定好规矩，让 AI 自己学会守规矩

----

## Constitutional AI 如何自我审查

想象一个学生自己批改作业：

1. 先写出回答
2. 翻开“宪法”（一套规则：“这有帮助吗？诚实吗？会不会造成伤害？”）
3. 自我批评：“这个回答可能有误导性，因为……”
4. 根据批评修改回答
5. 反复修改，直到通过所有规则

AI 变成了自己的老师——比雇几千个人类审核员便宜太多了！

----

## DPO：更简洁的路

RLHF 步骤多、调起来费劲——要训练打分系统、调整大语言模型、还有一堆细节要操心……

DPO 另辟蹊径：

- 完全省掉奖励模型这一步
- 直接让大语言模型学会“好答案优先于坏答案”
- 效果差不多，流程却简单了很多

> [DPO](https://arxiv.org/abs/2305.18290)（2023）— 一个公式干掉整条 RLHF 流水线

----

## DPO vs RLHF：简化流水线

| | RLHF | DPO |
|---|---|---|
| 步骤 | 3 步（SFT → 奖励模型 → RL） | 1 步（直接优化） |
| 需要额外大语言模型 | 是（奖励模型） | 不需要 |
| 稳定性 | 调参困难 | 更稳定 |
| 效果 | 很好 | 基本持平 |

DPO 的核心洞察：不需要单独的奖励模型，大语言模型自己就能从“好回答 vs 坏回答”的配对中直接学会偏好。

----

## GRPO & DAPO：把 RL 训练做大

大语言模型越大，用奖励来训练就越容易出问题：

- **GRPO** ：把一组回答放在一起比较，选出最好的——不需要额外的打分系统。DeepSeek-R1 靠的就是这招
- **DAPO** ：四个小妙招让超大模型的训练保持稳定，训出了跟 R1 一样强的推理能力

> [GRPO](https://arxiv.org/abs/2402.03300)（2024）| [DAPO](https://arxiv.org/abs/2503.14476)（2025）

----

## 自我博弈 (Self-Play)：自己跟自己练

大语言模型能不能通过 **跟自己对弈** 来变强？

- 当前大语言模型生成回答
- 一个副本试着分辨“哪些是真实回答、哪些是大语言模型编的”
- 大语言模型努力让回答越来越逼真 → 回答质量越来越高

就像棋手自己跟自己下棋，越下越强。

> [SPIN](https://arxiv.org/abs/2401.01335)（2024）— 自我对弈让弱大语言模型变强

----

## 不只看答案，更要看过程

想象批改一张数学试卷，哪种反馈更有效？

- **“这道题答错了”** （结果监督）
- **“第 3 步算错了”** （过程监督）

显然，指出具体哪一步出了问题，对大语言模型的帮助更大。

> [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)（2023）— 过程奖励优于结果奖励

----

## 第 6 章总结

| 论文 | 核心思想 |
|------|----------|
| [RLHF (Anthropic)](https://arxiv.org/abs/2204.05862)（2022） | 人类给回答排名 → 大语言模型学会偏好 |
| [Constitutional AI](https://arxiv.org/abs/2212.08073)（2022） | 用原则让 AI 自我审查 |
| [DPO](https://arxiv.org/abs/2305.18290)（2023） | 跳过奖励模型，直接优化偏好 |
| [GRPO](https://arxiv.org/abs/2402.03300) / [DAPO](https://arxiv.org/abs/2503.14476) | 大规模 RL 对齐训练 |
| [SPIN](https://arxiv.org/abs/2401.01335)（2024） | 自我博弈 → 自我提升 |
| [Step by Step](https://arxiv.org/abs/2305.20050)（2023） | 检查过程比只看答案更有效 |

----

## 想深入了解？

- [RLHF Explained (Hugging Face)](https://huggingface.co/blog/rlhf) — 逐步讲解 RLHF 的工作流程
- [Constitutional AI Explained (Anthropic)](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback) — AI 如何实现自我监督
- [DPO Paper Walkthrough (Interconnects)](https://www.interconnects.ai/p/the-dpo-cycle) — 为什么很多实验室用 DPO 替代了 RLHF
- [AI Safety for Kids (80,000 Hours)](https://80000hours.org/problem-profiles/artificial-intelligence/) — 为什么 AI 安全对未来至关重要

----

## 想一想

- 如果让你给 AI 写一份“宪法”，你会列出哪三条最重要的规则？
- RLHF 靠人类来排名，但人类自己有时候意见也不一致。碰到争议话题，大语言模型该怎么办？
- 自我博弈让大语言模型跟自己对练来变强。这会不会出问题？如果它学到了看起来很好但实际上没用的“花招”呢？
- 过程监督检查每一步，而不只是看最终答案。你更希望老师只改期末考试卷，还是每天都检查作业？为什么？

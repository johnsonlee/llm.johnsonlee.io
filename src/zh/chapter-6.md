<!-- .slide: class="center" -->

## 6. 对齐与安全

*怎么让 AI 既有用又无害？*

----

## 问题：AI 只是在“学舌”

一个靠预测下一个词训练出来的大语言模型，会输出 **概率最高的话** ——这意味着它也可能说出：

- 带偏见、令人不适甚至有害的内容
- 听起来很自信的假话（幻觉）
- 完全跑偏、答非所问的回答

> 怎样才能让它既 **有帮助** ，又不 **闯祸** ？

----

## RLHF：请人类来当老师

思路：就像教孩子一样，用 **人类的反馈** 来引导模型！

1. 让模型同时生成好几个回答
2. 人类把这些回答 **排个名次**
3. 用排名数据训练一个“奖励模型”，学会分辨好坏
4. 再训练语言模型去追求更高的奖励

> [Training a Helpful and Harmless Assistant](https://arxiv.org/abs/2204.05862)（2022）— Anthropic 提出的 RLHF 方法

----

## Constitutional AI：让 AI 自己管自己

收集人类反馈 **又贵又慢** 。能不能让 AI 自己给自己打分？

- 给 AI 定一套 **行为准则** （相当于一部“宪法”）
- “要有帮助、要诚实、不能伤害人”
- AI 根据这些准则审视并修改自己的回答

> [Constitutional AI](https://arxiv.org/abs/2212.08073)（2022）— 定好规矩，让 AI 自己学会守规矩

----

## DPO：更简洁的路

RLHF 步骤多、调起来费劲——要训练打分系统、调整模型、还有一堆细节要操心……

DPO 另辟蹊径：

- 完全省掉奖励模型这一步
- 直接让模型学会“好答案优先于坏答案”
- 效果差不多，流程却简单了很多

> [DPO](https://arxiv.org/abs/2305.18290)（2023）— 一个公式干掉整条 RLHF 流水线

----

## GRPO & DAPO：把 RL 训练做大

模型越大，用奖励来训练就越容易出问题：

- **GRPO** ：把一组回答放在一起比较，选出最好的——不需要额外的打分系统。DeepSeek-R1 靠的就是这招
- **DAPO** ：四个小妙招让超大模型的训练保持稳定，训出了跟 R1 一样强的推理能力

> [GRPO](https://arxiv.org/abs/2402.03300)（2024）| [DAPO](https://arxiv.org/abs/2503.14476)（2025）

----

## 自我博弈 (Self-Play)：自己跟自己练

模型能不能通过 **跟自己对弈** 来变强？

- 当前模型生成回答
- 一个副本试着分辨“哪些是真实回答、哪些是模型编的”
- 模型努力让回答越来越逼真 → 回答质量越来越高

就像棋手自己跟自己下棋，越下越强。

> [SPIN](https://arxiv.org/abs/2401.01335)（2024）— 自我对弈让弱模型变强

----

## 不只看答案，更要看过程

想象批改一张数学试卷，哪种反馈更有效？

- **“这道题答错了”** （结果监督）
- **“第 3 步算错了”** （过程监督）

显然，指出具体哪一步出了问题，对模型的帮助更大。

> [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)（2023）— 过程奖励优于结果奖励

----

## 第 6 章总结

| 论文 | 核心思想 |
|------|----------|
| [RLHF (Anthropic)](https://arxiv.org/abs/2204.05862)（2022） | 人类给回答排名 → 模型学会偏好 |
| [Constitutional AI](https://arxiv.org/abs/2212.08073)（2022） | 用原则让 AI 自我审查 |
| [DPO](https://arxiv.org/abs/2305.18290)（2023） | 跳过奖励模型，直接优化偏好 |
| [GRPO](https://arxiv.org/abs/2402.03300) / [DAPO](https://arxiv.org/abs/2503.14476) | 大规模 RL 对齐训练 |
| [SPIN](https://arxiv.org/abs/2401.01335)（2024） | 自我博弈 → 自我提升 |
| [Step by Step](https://arxiv.org/abs/2305.20050)（2023） | 检查过程比只看答案更有效 |

<!-- .slide: class="center" -->

## 16. 数据工程与评估

*怎么知道 AI 到底聪不聪明？*

----

## 垃圾进，垃圾出

模型的上限取决于训练数据的质量。可互联网上到处是：

- 重复内容（同一段文字被复制成千上万次）
- 低质量文本（垃圾邮件、广告、乱码）
- 有害内容（仇恨言论、虚假信息）

> 怎么从浩瀚的互联网里淘出 **最优质** 的训练数据？

----

## FineWeb：清洗 15 万亿词

HuggingFace 搭了一条大规模数据清洗流水线：

1. 下载整个互联网的网页数据（Common Crawl）
2. 去重——同样的内容只保留一份
3. 质量筛选——留下写得好的、有信息量的文本
4. 过滤有害内容
5. 最终产出： **15 万亿词** 的高质量训练数据，全部免费开源

> [FineWeb](https://arxiv.org/abs/2406.17557)（2024）— 开源训练数据的黄金标准

----

## 训完了，怎么考它？

模型训好了，到底有多聪明？需要 **基准测试 (Benchmarks)** 来衡量——相当于给 AI 出一套标准化考题。

- 就像学校用考试来评估学生
- 每套基准考察不同的能力维度
- 关键是：题目必须是模型在训练中 **从未见过** 的

----

## MMLU：包罗万象的知识大考

覆盖 57 门课——从历史到科学再到法律：

- 小学数学、世界历史、计算机科学、医学……应有尽有
- 全是选择题，像一场超大规模的综合考试
- 考的是知识 **广度** ——模型能不能当一个全科通才

> [MMLU](https://arxiv.org/abs/2009.03300)（2020）— 57 个学科、数千道题

----

## GPQA：连博士都觉得难

MMLU 考的是知识面广不广，GPQA 考的是 **有多深** ：

- 物理、化学、生物的研究生级别难题
- 难到普通人就算开着搜索引擎也答不出来
- 就连该领域的专家正确率也只有约 65%

> [GPQA](https://arxiv.org/abs/2311.12022)（2023）— “搜都搜不到答案”的专家级问答

----

## IFEval：你让它干什么，它就干什么吗？

光聪明还不够，关键是模型能不能 **严格照你说的做** ：

- “写一首恰好 4 节的诗” → 真的写了 4 节吗？
- “用 JSON 格式回复” → 输出的是合法 JSON 吗？
- “全文不要出现字母 e” → 有没有不小心漏出来？

衡量的是模型对指令的 **精确执行能力** 。

> [IFEval](https://arxiv.org/abs/2311.07911)（2023）— 指令遵循能力评估

----

## ARC-AGI：AI 的智商测试

终极问题：AI 能不能推理 **从没见过的** 全新事物？

- 一组抽象视觉谜题——很像智商测试里的图形推理
- 每道题需要从几个例子中 **归纳出一条新规则**
- 人类能轻松解出大部分，AI 却一筹莫展
- 至今基本没有被攻克——留给未来 AI 的挑战

> [ARC-AGI](https://arxiv.org/abs/1911.01547)（2019）— 抽象推理，AI 至今最大的短板

----

## 第 16 章总结

| 论文 | 核心思想 |
|------|----------|
| [FineWeb](https://arxiv.org/abs/2406.17557)（2024） | 15 万亿 token 的开源高质量数据 |
| [MMLU](https://arxiv.org/abs/2009.03300)（2020） | 57 学科综合知识考试 |
| [GPQA](https://arxiv.org/abs/2311.12022)（2023） | 搜都搜不到的专家级难题 |
| [IFEval](https://arxiv.org/abs/2311.07911)（2023） | 测试指令执行的精确度 |
| [ARC-AGI](https://arxiv.org/abs/1911.01547)（2019） | 抽象推理——至今未攻克 |

<!-- .slide: class="center" -->

## 16. 数据工程与评估

*怎么知道 AI 到底聪不聪明？*

----

## 垃圾进，垃圾出

大语言模型的上限取决于训练数据的质量。可互联网上到处是：

- 重复内容（同一段文字被复制成千上万次）
- 低质量文本（垃圾邮件、广告、乱码）
- 有害内容（仇恨言论、虚假信息）

> 怎么从浩瀚的互联网里淘出 **最优质** 的训练数据？

----

## FineWeb：清洗 15 万亿词

HuggingFace 搭了一条大规模数据清洗流水线：

1. 下载整个互联网的网页数据（Common Crawl）
2. 去重——同样的内容只保留一份
3. 质量筛选——留下写得好的、有信息量的文本
4. 过滤有害内容
5. 最终产出： **15 万亿词** 的高质量训练数据，全部免费开源

> [FineWeb](https://arxiv.org/abs/2406.17557)（2024）— 开源训练数据的黄金标准

----

## 为什么数据清洗这么难

想象你要把一个大海那么大的游泳池清理干净：

- **去重** ：同一篇维基百科文章被一万个网站转载了。不去重的话，大语言模型只会背答案，学不到东西。
- **质量** ：一篇好的博客文章 vs 一页全是广告的垃圾网页。怎么教程序分辨好坏？
- **有害内容** ：仇恨言论、虚假信息、个人隐私。必须去掉，但又不能删太多，否则大语言模型的知识面会变窄。

FineWeb 把 **15 万亿词** 过了这条清洗流水线——然后免费开放给所有人！

----

## 训完了，怎么考它？

大语言模型训好了，到底有多聪明？需要 **基准测试 (Benchmarks)** 来衡量——相当于给 AI 出一套标准化考题。

- 就像学校用考试来评估学生
- 每套基准考察不同的能力维度
- 关键是：题目必须是大语言模型在训练中 **从未见过** 的

----

## MMLU：包罗万象的知识大考

覆盖 57 门课——从历史到科学再到法律：

- 小学数学、世界历史、计算机科学、医学……应有尽有
- 全是选择题，像一场超大规模的综合考试
- 考的是知识 **广度** ——大语言模型能不能当一个全科通才

> [MMLU](https://arxiv.org/abs/2009.03300)（2020）— 57 个学科、数千道题

----

## MMLU 的题目长什么样

来看几道不同学科的真题：

**天文学** ：“地球的四季是什么造成的？” (A) 与太阳的距离 (B) 地轴的倾斜 ✓ (C) 自转速度 (D) 月球大小

**法学** ：“根据美国宪法第四修正案，以下哪项不属于'搜查'？” ……

**医学** ：“一位 45 岁的患者出现以下症状……” ……

57 个学科，从小学水平到研究生水平都有。能在 MMLU 上拿高分的大语言模型，才算真正的全科通才！

----

## GPQA：连博士都觉得难

MMLU 考的是知识面广不广，GPQA 考的是 **有多深** ：

- 物理、化学、生物的研究生级别难题
- 难到普通人就算开着搜索引擎也答不出来
- 就连该领域的专家正确率也只有约 65%

> [GPQA](https://arxiv.org/abs/2311.12022)（2023）— “搜都搜不到答案”的专家级问答

----

## IFEval：你让它干什么，它就干什么吗？

光聪明还不够，关键是大语言模型能不能 **严格照你说的做** ：

- “写一首恰好 4 节的诗” → 真的写了 4 节吗？
- “用 JSON 格式回复” → 输出的是合法 JSON 吗？
- “全文不要出现字母 e” → 有没有不小心漏出来？

衡量的是大语言模型对指令的 **精确执行能力** 。

> [IFEval](https://arxiv.org/abs/2311.07911)（2023）— 指令遵循能力评估

----

## ARC-AGI：AI 的智商测试

终极问题：AI 能不能推理 **从没见过的** 全新事物？

- 一组抽象视觉谜题——很像智商测试里的图形推理
- 每道题需要从几个例子中 **归纳出一条新规则**
- 人类能轻松解出大部分，AI 却一筹莫展
- 至今基本没有被攻克——留给未来 AI 的挑战

> [ARC-AGI](https://arxiv.org/abs/1911.01547)（2019）— 抽象推理，AI 至今最大的短板

----

## 为什么 ARC-AGI 对大语言模型来说这么难

ARC 的谜题在人类看来并不复杂：

- 输入：3 个有颜色的方格图展示一种规律 → 输出：第 4 个方格图应该是什么样的？
- 人类几秒钟就能看出规律——我们凭直觉就能“领悟”
- 大语言模型却很费劲，因为每道题都是一条 **它从没见过的新规则**

这正是 **记忆模式** （大语言模型的强项）和 **理解抽象规则** （人类的天赋）之间的差距。ARC-AGI 考的不是“背”，而是“真正会推理”。

----

## 第 16 章总结

| 论文 | 核心思想 |
|------|----------|
| [FineWeb](https://arxiv.org/abs/2406.17557)（2024） | 15 万亿 token 的开源高质量数据 |
| [MMLU](https://arxiv.org/abs/2009.03300)（2020） | 57 学科综合知识考试 |
| [GPQA](https://arxiv.org/abs/2311.12022)（2023） | 搜都搜不到的专家级难题 |
| [IFEval](https://arxiv.org/abs/2311.07911)（2023） | 测试指令执行的精确度 |
| [ARC-AGI](https://arxiv.org/abs/1911.01547)（2019） | 抽象推理——至今未攻克 |

----

## 想深入了解？

- [FineWeb Dataset (Hugging Face)](https://huggingface.co/datasets/HuggingFaceFW/fineweb) — 亲自探索这个 15 万亿 token 的数据集
- [MMLU Leaderboard (Papers With Code)](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) — 看看哪些大语言模型在知识考试中得分最高
- [ARC-AGI Prize](https://arcprize.org/) — 自己试着做 ARC 谜题，还能争夺 100 万美元大奖
- [Chatbot Arena (LMSYS)](https://chat.lmsys.org/) — 在盲测中投票选出更好的大语言模型

----

## 想一想

- FineWeb 去掉了互联网上的重复内容。但有些文字重复出现恰恰因为它很重要（比如名人名言）。你怎么判断哪些是“坏的重复”、哪些是“有用的重复”？
- MMLU 用选择题考了 57 个学科。选择题是衡量智力的好方式吗？有哪些重要能力是选择题测不出来的？
- ARC-AGI 的谜题对人类很简单，对 AI 却很难。你能想到反过来的例子吗——AI 觉得简单但人类觉得难的任务？
- 如果让你设计一套全新的 AI 评测基准，你会测些什么是现有基准没测到的？

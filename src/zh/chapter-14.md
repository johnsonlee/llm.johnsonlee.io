<!-- .slide: class="center" -->

## 14. 推理模型

*AI 到底是在“想”，还是只是在“猜”？*

----

## “猜”和“想”的区别

普通的大语言模型只是在预测 **概率最大的下一个词** 。而真正的推理需要：

- 把问题拆成几步来处理
- 尝试不同的思路
- 回头检查自己的过程
- 走不通时退回来换条路

> 我们能造出真正会 **动脑筋** 的模型，而不只是在做模式匹配吗？

----

## 推理方法的全景图

研究人员系统梳理了构建推理模型的各种路线：

- **思维链 (Chain-of-Thought)** ：一步一步想（第 7 章讲过）
- **搜索** ：同时试多条路（思维树）
- **强化学习** ：做对了有奖励，做错了受惩罚，慢慢学会
- **自我改进** ：模型不断优化自己的推理能力

> [Towards Large Reasoning Models: A Survey](https://arxiv.org/abs/2501.09686)（2025）— 推理模型的全面综述

----

## 推理模型的“施工图”

具体怎么从零开始 **打造** 一个推理模型？

1. 找一个底子够硬的基础模型
2. 收集一批附带详细解题过程的训练数据
3. 用强化学习训练——推理正确就奖励，犯错就惩罚
4. 模型逐渐学会在回答前先写出 **思考步骤**

> [Reasoning Language Models: A Blueprint](https://arxiv.org/abs/2501.11223)（2025）— 手把手教你造推理模型

----

## s1：其实不用花那么多钱

OpenAI 的 o1 看起来又贵又复杂。能不能用更低成本复现？

- 只需要 1,000 条精心挑选的训练样本（不是几百万条！）
- 方法很直白：难题就让模型“想久一点”
- 效果竟然跟那些砸重金训出来的方案差不多

> [s1: Simple Test-Time Scaling](https://arxiv.org/abs/2501.19393)（2025）— 少而精，简单有效

----

## Kimi k1.5：稳住 RL 训练

用强化学习训推理模型不容易，模型可能学歪：

- **钻奖励的空子** ：找到偷偷拿高分的歪门邪道，但其实根本没真正理解题目
- **训练不稳定** ：参数动不动就炸了或者发散了
- Kimi k1.5 引入了一系列新的 RL 技术来让训练过程保持稳定

> [Kimi k1.5](https://arxiv.org/abs/2501.12599)（2025）— 让推理模型的 RL 训练更稳

----

## LIMO：要质量不要数量

传统认知：训练数据越多，推理能力越强。

LIMO 的发现恰恰相反：

- 底子好的模型其实只需要 **很少的** 推理示例
- 示例的质量远比数量重要
- 几百条精品数据就能激发出强大的推理能力

> [LIMO](https://arxiv.org/abs/2502.03387)（2025）— “少即是多”假说

----

## 模型“想了半天”到底在干嘛？

当模型产生一长串思维链时，它内部到底在做什么？

- 研究人员分析了数千条长推理过程
- 找到了一些共性模式：探索、回溯、验证、归纳
- 想得越久效果越好——但超过一定限度后，质量反而会下降

> [Demystifying Long Chain-of-Thought](https://arxiv.org/abs/2502.03373)（2025）— 揭开长思维链的面纱

----

## 第 14 章总结

| 论文 | 核心思想 |
|------|----------|
| [Reasoning Survey](https://arxiv.org/abs/2501.09686)（2025） | 推理方法的全面综述 |
| [Blueprint](https://arxiv.org/abs/2501.11223)（2025） | 推理模型的建造指南 |
| [s1](https://arxiv.org/abs/2501.19393)（2025） | 简单低成本的测试时缩放 |
| [Kimi k1.5](https://arxiv.org/abs/2501.12599)（2025） | 让推理的 RL 训练更稳定 |
| [LIMO](https://arxiv.org/abs/2502.03387)（2025） | 几百条精品数据胜过海量平庸数据 |
| [Demystifying](https://arxiv.org/abs/2502.03373)（2025） | 长思维链里到底发生了什么 |

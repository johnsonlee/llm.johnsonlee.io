<!-- .slide: class="center" -->

## 14. 推理模型

*AI 到底是在“想”，还是只是在“猜”？*

----

## “猜”和“想”的区别

普通的大语言模型只是在预测 **概率最大的下一个词** 。而真正的推理需要：

- 把问题拆成几步来处理
- 尝试不同的思路
- 回头检查自己的过程
- 走不通时退回来换条路

> 我们能造出真正会 **动脑筋** 的大语言模型，而不只是在做模式匹配吗？

----

## 推理方法的全景图

研究人员系统梳理了构建推理模型的各种路线：

- **思维链 (Chain-of-Thought)** ：一步一步想（第 7 章讲过）
- **搜索** ：同时试多条路（思维树）
- **强化学习** ：做对了有奖励，做错了受惩罚，慢慢学会
- **自我改进** ：大语言模型不断优化自己的推理能力

> [Towards Large Reasoning Models: A Survey](https://arxiv.org/abs/2501.09686)（2025）— 推理模型的全面综述

----

## 推理模型的“施工图”

具体怎么从零开始 **打造** 一个推理模型？

1. 找一个底子够硬的基础模型
2. 收集一批附带详细解题过程的训练数据
3. 用强化学习训练——推理正确就奖励，犯错就惩罚
4. 大语言模型逐渐学会在回答前先写出 **思考步骤**

> [Reasoning Language Models: A Blueprint](https://arxiv.org/abs/2501.11223)（2025）— 手把手教你造推理模型

----

## 推理模型的训练配方

制作一个推理模型的步骤：

1. **食材** ：一个底子好的基础模型 + 几千道附带详细解题过程的题目
2. **冷启动** ：先用一批人类写的推理示例做监督微调
3. **RL 训练** ：给新题目，推理正确就奖励，出错就惩罚
4. **魔法时刻** ：慢慢地，大语言模型学会了在回答之前先写“让我想想……”

关键发现：这样训练出来的大语言模型会发展出训练数据中 **没有的** 全新推理策略——真正的举一反三！

----

## s1：其实不用花那么多钱

OpenAI 的 o1 看起来又贵又复杂。能不能用更低成本复现？

- 只需要 1,000 条精心挑选的训练样本（不是几百万条！）
- 方法很直白：难题就让大语言模型“想久一点”
- 效果竟然跟那些砸重金训出来的方案差不多

> [s1: Simple Test-Time Scaling](https://arxiv.org/abs/2501.19393)（2025）— 少而精，简单有效

----

## 为什么 1,000 条数据就够了

这听起来不可思议——1,000 条怎么能跟几百万条比？

秘诀在于 **质量远胜数量** 。

- 每条数据都经过精心挑选，覆盖一种不同的推理模式
- 大语言模型学的不是背答案，而是 **怎么思考**
- 就像学国际象棋：你不需要背下每一局棋谱。几百个精心分析过的局面就能教会你基本原则。

s1 证明了：小而精的数据集 + 聪明的推理时间分配，就能媲美那些昂贵的方案。

----

## Kimi k1.5：稳住 RL 训练

用强化学习训推理模型不容易，大语言模型可能学歪：

- **钻奖励的空子** ：找到偷偷拿高分的歪门邪道，但其实根本没真正理解题目
- **训练不稳定** ：参数动不动就炸了或者发散了
- Kimi k1.5 引入了一系列新的 RL 技术来让训练过程保持稳定

> [Kimi k1.5](https://arxiv.org/abs/2501.12599)（2025）— 让推理模型的 RL 训练更稳

----

## LIMO：要质量不要数量

传统认知：训练数据越多，推理能力越强。

LIMO 的发现恰恰相反：

- 底子好的大语言模型其实只需要 **很少的** 推理示例
- 示例的质量远比数量重要
- 几百条精品数据就能激发出强大的推理能力

> [LIMO](https://arxiv.org/abs/2502.03387)（2025）— “少即是多”假说

----

## 大语言模型“想了半天”到底在干嘛？

当大语言模型产生一长串思维链时，它内部到底在做什么？

- 研究人员分析了数千条长推理过程
- 找到了一些共性模式：探索、回溯、验证、归纳
- 想得越久效果越好——但超过一定限度后，质量反而会下降

> [Demystifying Long Chain-of-Thought](https://arxiv.org/abs/2502.03373)（2025）— 揭开长思维链的面纱

----

## 好的推理过程长什么样

研究人员发现，高质量的推理链有一些共同模式：

- **探索** ：“让我试试方法 A…… 再考虑一下方法 B……”
- **回溯** ：“等等，这条路走不通。让我退回去换个思路。”
- **验证** ：“我来检查一下：我的答案满足所有条件吗？”
- **归纳** ：“到目前为止我得到了 X、Y、Z。综合起来看……”

但有个极限：当思考超过大约 32,000 个 token 后，质量反而会 **下降** ——大语言模型开始原地打转！

----

## 第 14 章总结

| 论文 | 核心思想 |
|------|----------|
| [Reasoning Survey](https://arxiv.org/abs/2501.09686)（2025） | 推理方法的全面综述 |
| [Blueprint](https://arxiv.org/abs/2501.11223)（2025） | 推理模型的建造指南 |
| [s1](https://arxiv.org/abs/2501.19393)（2025） | 简单低成本的测试时缩放 |
| [Kimi k1.5](https://arxiv.org/abs/2501.12599)（2025） | 让推理的 RL 训练更稳定 |
| [LIMO](https://arxiv.org/abs/2502.03387)（2025） | 几百条精品数据胜过海量平庸数据 |
| [Demystifying](https://arxiv.org/abs/2502.03373)（2025） | 长思维链里到底发生了什么 |

----

## 想深入了解？

- [How Reasoning Models Work (Interconnects)](https://www.interconnects.ai/p/openai-o1-and-reasoning) — 深入解析 o1 风格的推理模型
- [DeepSeek-R1 Technical Blog](https://api-docs.deepseek.com/news/news250120) — R1 如何通过纯 RL 学会推理
- [Test-Time Compute Explained](https://www.youtube.com/watch?v=UhPnC6MqB7E) — 为什么“多想一会儿”有用（视频）
- [s1: Simple Test-Time Scaling (Paper)](https://arxiv.org/abs/2501.19393) — “少即是多”的推理训练方案

----

## 想一想

- 推理模型通过写出思考步骤来“思考”。这算真正的思考，还是只是一种高级的模式匹配？两者的区别在哪儿？
- s1 证明了 1,000 条优质数据能媲美几百万条普通数据。你在学校有没有类似的经历——一个好的讲解比看一整本书都有用？
- 想太久（超过 32K token）反而会让推理变差。你觉得这是为什么？你考试时有没有过“想太多反而答错”的经历？
- 如果你能看到 AI 在给你答案之前所有的“思考步骤”，你会更信任它还是更不信任？为什么？

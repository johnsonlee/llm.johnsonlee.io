<!-- .slide: class="center" -->

## 13. 长上下文

*碰到特别特别长的文本怎么办？*

----

## 问题：记性不够长

大多数模型一次只能处理几千个字。

- 一封短邮件？轻松搞定
- 一本 300 页的书？塞不进去
- 一整个代码仓库？更别想了

> 怎么才能让 AI 读得了 **超长** 的文本？

----

## ALiBi：越远越淡化

一种直觉想法：让模型 **天然更关注附近的内容** ，离得远的就少关注一些。

- 不搞复杂的位置编码
- 直接按距离加一个 **小小的递减权重**
- 近处的词获得充分关注，远处的词被逐渐弱化
- 额外好处：能处理 **比训练时更长的文本**

> [ALiBi](https://arxiv.org/abs/2108.12409)（2021）— 靠简单的距离衰减，就能处理更长的文本

----

## YaRN：把绳子拉长

还记得 RoPE（旋转位置编码）吗？它有个固有的长度上限。

YaRN 想了个巧办法把它 **拉长** ：

- 调整旋转频率来适配更长的上下文
- 在 4,000 个词上训练的模型可以拉伸到能处理 **128,000 个词**
- 不需要从头重新训练

> [YaRN](https://arxiv.org/abs/2309.00071)（2023）— 把 RoPE 的长度极限拉伸

----

## RAG vs 长上下文：谁更好用？

等等——如果上下文能做得足够长，还需要 RAG 吗？

研究人员做了正面比较：

- **长上下文** ：把所有资料一股脑塞进去——方便但非常费算力
- **RAG** ：只搜最相关的部分——省算力但可能漏掉有用信息
- **结论** ：各有所长，得看具体任务来选

> [RAG vs Long Context](https://arxiv.org/abs/2407.16833)（2024）— 两种方案的正面对比

----

## 长上下文真的靠谱吗？

大语言模型号称能处理 10 万以上的 token，但它 **真的全看懂了** 吗？

- **LongBench v2** ：测试模型对长文本各个位置的理解程度
- **MRCR** ：在超长对话里，AI 能不能找到很久之前提过的一条信息？

很多模型在答案藏在文本 **中间** 位置时表现明显下滑！

> [LongBench v2](https://arxiv.org/abs/2412.15204)（2024）| [MRCR](https://arxiv.org/abs/2409.12640)（2024）

----

## 第 13 章总结

| 论文 | 核心思想 |
|------|----------|
| [ALiBi](https://arxiv.org/abs/2108.12409)（2021） | 按距离衰减注意力，实现长度外推 |
| [YaRN](https://arxiv.org/abs/2309.00071)（2023） | 拉伸 RoPE，支持更长上下文 |
| [RAG vs Long Context](https://arxiv.org/abs/2407.16833)（2024） | 两种方案各有优劣 |
| [LongBench v2](https://arxiv.org/abs/2412.15204)（2024） | 检验长上下文真实理解能力 |
| [MRCR](https://arxiv.org/abs/2409.12640)（2024） | 长对话中的多轮信息检索 |

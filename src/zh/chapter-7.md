<!-- .slide: class="center" -->

## 7. 提示工程与推理

*怎么跟 AI 说话，才能得到最好的回答？*

----

## 问对问题的魔力

同一个大语言模型，换一种问法，回答的质量天差地别：

> “17 × 24 等于多少？” → “408”（经常算错！）

> “17 × 24 等于多少？请一步一步算。” → 准确率飙升！

这门“如何写好提示词”的学问叫做 **提示工程 (Prompt Engineering)** 。

----

## 思维链 (Chain-of-Thought)：请写出解题过程

还记得数学老师常说“要写解题步骤”吗？

- 在提示词里给大语言模型看几个“写出推理过程”的例子
- 大语言模型就会学着 **先推导、再得出结论**
- 数学题的正确率大幅提升！

> [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)（2022）— 给几个示例，大语言模型就学会一步步推理

----

## 思维链：用与不用的差别

**不用思维链：**
> 问：“小明有 5 个苹果，又买了 2 袋，每袋 3 个。现在有几个？”
> 答：“11”（直接给答案，有时会算错）

**用思维链：**
> 问：同样的题。请一步步想。
> 答：“小明原来有 5 个苹果。他买了 2 袋，每袋 3 个。2 × 3 = 6。5 + 6 = 11。”

推理步骤帮助大语言模型 **自我检查逻辑** ，在推导过程中及时发现错误。

----

## 零样本 CoT：一句话就搞定

连示例都不用给！只需在问题后面加一句：

> **“让我们一步一步来想。”**

就这么简单。这一条指令就能激发大语言模型的推理能力。

- 对数学、逻辑、常识推理都有效
- 大概是最简单、又确实管用的提示技巧了

> [Zero-Shot CoT](https://arxiv.org/abs/2205.11916)（2022）— “一步一步来”的魔力

----

## 第一条路走不通怎么办？

思维链只沿一条路走到底。万一走进死胡同呢？

- **思维树 (Tree of Thoughts)** 像走迷宫一样同时探索多条路
- 每一步都展开几个分支、评估好坏，走不通就 **退回来换条路**
- 最终选出最靠谱的那条推理路径

> [Tree of Thoughts](https://arxiv.org/abs/2305.10601)（2023）— 探索、评估、回溯

----

## 思维树：像走迷宫一样解题

想象你在走迷宫：

- **思维链** = 选一条路一直走到底。要是死胡同，运气不好！
- **思维树** = 每到一个岔路口，派侦察兵去探每条路。侦察兵回来汇报。选最有希望的那条。走不通就退回来换一条。

这跟国际象棋引擎的思路一模一样——往前推演多步、砍掉差的分支、选出最佳策略。

----

## 自我一致性 (Self-Consistency)：多问几遍，投票定胜负

还有一招更直接的：同一个问题问大语言模型 **好多遍** ，让答案们投票！

- 每次大语言模型可能走不同的推理路径
- 最后看哪个答案 **出现次数最多** ，就选它
- 跟问 10 个朋友怎么走、听大多数人的意见是一个道理

> [Self-Consistency](https://arxiv.org/abs/2203.11171)（2022）— 多路采样 + 多数投票

----

## ReAct：边想边做

如果大语言模型碰到自己不知道的信息怎么办？

- **想** ：“我得查一下东京的人口”
- **做** ：搜索网络 → 拿到数据
- **想** ：“好，现在可以算了”
- **做** ：给出最终答案

把 **推理** 和 **行动** 交替进行——这正是 AI 智能体的雏形。

> [ReAct](https://arxiv.org/abs/2210.03629)（2022）— AI 智能体的蓝图

----

## ReAct 实战：一个真实例子

> **问题** ：“埃菲尔铁塔所在国家的人口是多少？”

1. **想** ：“我得先查清楚埃菲尔铁塔在哪个国家”
2. **做** ：搜索 → “埃菲尔铁塔在法国巴黎”
3. **想** ：“现在需要查法国的人口”
4. **做** ：搜索 → “法国约有 6800 万人”
5. **想** ：“我已经有答案了”
6. **做** ：回答 → “大约 6800 万”

两步搜索、三次思考——“想—做”交替循环，就把一个复杂问题拆解了！

----

## 测试时算力 (Test-Time Compute)：难题多想想

并不是每道题都一样难，为什么要花一样多的时间？

- 简单的问题：快速回答
- 复杂的问题：多花一些 **思考时间** 来仔细推演
- 按照题目难度动态分配思考时间

这正是 OpenAI o1 大语言模型背后的理论基础。

> [Scaling LLM Test-Time Compute](https://arxiv.org/abs/2408.03314)（2024）— 难题上多花功夫

----

## STaR：大语言模型自己教自己推理

大语言模型能不能自我提升推理能力？

1. 先试着用推理来解题
2. 把答对了的推理过程 **留下来**
3. 拿这些“好推理”去做进一步训练 → 大语言模型变得更强
4. 重复以上步骤

> [STaR](https://arxiv.org/abs/2203.14465)（2022）— 靠自己的成功经验越推越好

----

## 第 7 章总结

| 论文 | 核心思想 |
|------|----------|
| [Chain-of-Thought](https://arxiv.org/abs/2201.11903)（2022） | 给几个示例，大语言模型就学会一步步推理 |
| [Zero-Shot CoT](https://arxiv.org/abs/2205.11916)（2022） | 一句“一步一步来”就够了 |
| [Tree of Thoughts](https://arxiv.org/abs/2305.10601)（2023） | 同时探索多条路，走不通就回退 |
| [Self-Consistency](https://arxiv.org/abs/2203.11171)（2022） | 多问几遍，投票选答案 |
| [ReAct](https://arxiv.org/abs/2210.03629)（2022） | 边想边做，智能体的雏形 |
| [Test-Time Compute](https://arxiv.org/abs/2408.03314)（2024） | 难题多想想，别急着答 |
| [STaR](https://arxiv.org/abs/2203.14465)（2022） | 大语言模型从自己的成功推理中学习 |

----

## 想深入了解？

- [Prompt Engineering Guide (DAIR.AI)](https://www.promptingguide.ai/) — 最全面的提示工程学习资源
- [Chain-of-Thought Hub (GitHub)](https://github.com/FranxYao/chain-of-thought-hub) — 思维链研究合集
- [Tree of Thoughts Explained (Yannic Kilcher)](https://www.youtube.com/watch?v=ut5kp56wW_4) — 视频详解思维树及示例
- [ReAct Pattern (LangChain)](https://python.langchain.com/docs/modules/agents/agent_types/react) — 动手搭一个 ReAct 智能体

----

## 想一想

- “让我们一步一步来想”这句话能大幅提升大语言模型的表现。你觉得为什么仅仅加几个字就有这么大的效果？
- 自我一致性会把同一个问题问很多遍，选出现最多的答案。什么情况下，多数答案也可能是错的？
- ReAct 让大语言模型去网上搜索信息。如果搜到的信息本身就有错误或偏见，会带来什么风险？
- 如果让你设计一种全新的提示技巧，你会怎么设计？你会怎样帮助大语言模型解决它目前不擅长的问题？

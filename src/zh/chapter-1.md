<!-- .slide: class="center" -->

## 1. Transformer 基础

*大语言模型是怎么理解语言的？*

----

## 你是怎么理解一句话的？

读到“小猫坐在垫子上”，你一眼就能看出：

- **谁** 在坐？ — 小猫
- 坐在 **哪儿** ？ — 垫子上
- 在干 **什么** ？ — 坐着

你之所以能理解，是因为你 **一眼就看到了所有的字** ，并且自然地把它们的关系联系了起来。

----

## 大语言模型能做到吗？

早期的做法是 **逐字阅读** ，就像流水线一样：

> “小” → “猫” → “坐” → “在” → “垫” → “子” → “上”

问题是：读到“上”的时候，前面的“小猫”早就忘了！

- 记忆太短，长句子一多就容易出错
- 速度也慢，只能一个字一个字处理，没法并行

----

## 大语言模型眼里的“字”是什么？

大语言模型不像我们这样认字。它有一个专门的“切字工具”叫 **分词器 (tokenizer)** 。

分词器负责把文本切成一个个小片段，每个片段叫做 **token** ：

- “unhappiness” → “un” + “happiness”（2 个 token）
- “我喜欢吃西瓜” → “我” + “喜欢” + “吃” + “西瓜”（4 个 token）

----

## 分词器怎么决定从哪儿切？

- 在大语言模型训练之前，分词器会先扫描 **训练数据** （用来训练大语言模型的大量文本），统计哪些片段出现得最频繁，建立一份固定的”词表”
- 之后无论用户输入什么，都按这份词表来切
- “happiness” 出现频率很高 → 保留完整
- “unhappiness” 比较少见 → 拆成 “un” + “happiness”，两个分词器已经认识的碎片
- 中文也一样：“喜欢”“西瓜” 经常一起出现，保留为整体；“吃” 本身就是一个字，单独成一个 token

----

## 分词器是怎么训练出来的？

最常见的方法叫 **字节对编码 (Byte Pair Encoding, BPE)** ，核心思路像拼拼图——反复把最常相邻出现的一对合并成一个新片段：

1. 一开始，词表里只有最小单位（单个字母或单个字）
2. 扫描训练数据，找出最常相邻出现的一对，比如“t”和“h”
3. 把它们合并成“th”，加入词表
4. 反复合并：“th”+“e” → “the”，“i”+“ng” → “ing” …
5. 达到预设词表大小（比如 50,000 个）就停

> 分词器的训练在大语言模型训练 **之前** 完成。词表一旦固定，后面所有文本都按同一份词表来切。

----

## 为什么叫“字节对”？中文也能用吗？

**为什么叫“字节对”：** 这个算法最初是 1994 年为数据压缩发明的，当时确实是在字节层面合并最常见的相邻对，所以叫 Byte Pair Encoding。后来被 NLP（Natural Language Processing, 自然语言处理）借用，操作对象从字节变成了字符/子词，但名字沿用了下来。

**中文也用 BPE 吗？** 是的！主流大语言模型（GPT、LLaMA 等）对中文也用 BPE，只是起点不同：

- **英文** ：从单个字母 a, b, c 开始合并
- **中文** ：从单个字（或 UTF-8 字节）开始合并 —“喜”+“欢” → “喜欢”

合并逻辑完全一样：统计频率最高的相邻对，合并，重复。

----

## 词嵌入 (Embedding)：把 token 变成数字

大语言模型内部全是数学运算——但文字不是数字。怎么让大语言模型”看懂”文字呢？

- 给每个 token **一串数字** 来代表它的含义——这串数字叫 **向量 (vector)** ，其实就是“一串数字排成一行”
- 这个“token → 向量”的转换就叫 **词嵌入 (Embedding)**
- 含义相近的 token，向量也很接近——“猫”和“狗”比“猫”和“桌子”更像

> 词嵌入是大语言模型的“字典”：查一下 token，就能拿到对应的向量。

----

## 词嵌入是什么样的？

![词嵌入示意图](src/zh/images/embedding.svg)

----

## 向量有多长？

示意图里每个向量只有 4 个数字，但真正的大语言模型用的向量要长得多（BERT、GPT 是几个著名的大语言模型，后面会详细介绍）：

| 大语言模型 | 向量长度（维度 d） |
|---|---|
| BERT-base | 768 |
| GPT-2 | 1,024 |
| GPT-3 | 12,288 |

这串数字的长度叫做 **维度 d** ，是设计模型时 **人为定好的** ——维度越高，能表达的含义就越丰富，就像用更多科目来描述一个学生，画像就更立体。

> d 维空间在训练前就存在，但一开始是空的；训练的过程就是把每个 token“归置”到合理的位置上。

----

## 这些数字是怎么来的？

一开始，每个 token 的向量是 **随机** 的。训练时，大语言模型不断读句子、猜词、从错误中学习——向量就被一点点调准了。

假设大语言模型读了大量句子后发现：

- “我喂___吃鱼”“我喂___吃肉” —“猫”和“狗”都能填 → 向量越拉越近
- “我把___擦干净” — 只有“桌子”能填，“猫”“狗”填不了 → 向量越推越远

> 经常出现在相似位置的 token，向量就会变得相似——词嵌入不是人工设计的，是大语言模型自己“学”出来的。

----

## 为什么叫”Transformer”？

![Transformer 逐层变换](src/zh/images/transformer-layers.svg)

----

<!-- .slide: class=”center” -->

## 假如大语言模型能一眼看到每个 token 呢？

----

## 注意力机制 (Attention)：全部一起看

**注意力机制** 就是干这个的！

- 每个 token 都会“扫一眼”其他所有 token，看看谁跟自己最相关
- “坐”看到“猫” → 谁在坐？
- “坐”看到“垫子” → 坐在哪儿？
- 所有 token **同时处理** ，不用排队等

> 这个思路来自 [Attention Is All You Need](https://arxiv.org/abs/1706.03762)（2017），正是这篇论文提出了 **Transformer** 架构

----

## 注意力到底怎么运作的？

每个 token 都会产生三样东西：

1. **查询 (Query)** — “我在找什么？”
2. **键 (Key)** — “我能提供什么线索？”
3. **值 (Value)** — “我携带的实际信息是什么？”

大语言模型拿每个 token 的 Query 去匹配所有 token 的 Key，找到最相关的 token，再把它们的 Value 组合起来。就像上课举手提问（Query），看看同学们的名牌（Key），然后听对的人回答（Value）。

----

## Q、K、V 长什么样？

前面学过，每个 token 的 embedding 是一串 d 个数字。大语言模型用三组不同的权重把同一个 embedding 变成三份”成绩单”：

- **Q（查询）** — d 个数字：”我想找什么样的搭档？”
- **K（键）** — d 个数字：”我自己能提供什么？”
- **V（值）** — d 个数字：”我携带的实际内容”

**分工** ：Q 和 K 负责配对打分（”该关注谁”），V 是最终被混合的内容。这里简化到 **d=4** ——4 项分别代表：谁在做、做什么、在哪里、什么东西。

----

## 用“成绩单”表示每个 token

| | 谁在做 | 做什么 | 在哪里 | 什么东西 |
|---|---|---|---|---|
| 猫 Q（想找） | 1 | 1 | 1 | 1 |
| 猫 K（能给） | 1 | 0 | 0 | 0 |
| 坐 K（能给） | 1 | 1 | 1 | 1 |
| 垫 K（能给） | 0 | 0 | 0 | 1 |

这些数字代表什么？下一页来看看。

----

## 为什么是这些数字？

- **猫 Q** 全是 1：“猫”刚出现，大语言模型还不知道它在干什么、在哪里，所以每个方面都想打听
- **坐 K** 全是 1：“坐”是个动词，包含动作、主体、地点等丰富信息——跟猫 **超合拍** ！
- **垫 K** 只有“什么东西”是 1：“垫”主要提供“事物”信息——跟猫 **不太合拍**

> 注意：这些数字是为了方便演示而简化的。真正的大语言模型会在训练中 **自动学会** 每个 token 的 Q、K 和 V。

怎么用数学算出“合拍程度”呢？

----

## 点积：比较两张“成绩单”

方法叫 **点积** ——把每一项评分对应相乘，再全部加起来。

就像配对交朋友：共同爱好越多（分数都高的项越多），总分就越大——越合拍！来算“猫”和每个 token 有多合拍：

- 猫Q · 猫K = 1×1 + 1×0 + 1×0 + 1×0 = **1** — 一般
- 猫Q · 坐K = 1×1 + 1×1 + 1×1 + 1×1 = **4** — 超合拍！
- 猫Q · 垫K = 1×0 + 1×0 + 1×0 + 1×1 = **1** — 一般

“猫”和“坐”的点积最大——“猫”最关注“坐”！

----

## 从分数到百分比

上一步用 Q 和 K 得到相关性分数 [1, 4, 1]，怎么把它变成”该关注谁”的百分比？

**第 1 步：缩一缩** — 分数 ÷ 2 → [0.5, 2.0, 0.5]
分数太大会出问题（后面会专门解释），先”降降温”

**第 2 步：画饼图 (Softmax)** → **[15%, 69%, 15%]**
把分数变成百分比——就像画饼图，所有比例加起来正好 100%

到这里，Q 和 K 的任务完成了——它们算出”猫”应该 69% 关注”坐”，各 15% 关注”猫”和”垫”。接下来轮到 V 登场！

----

## 用 V 混合出最终结果

Q 和 K 算出了”该关注谁”，现在用 V 来混合每个 token 的 **实际内容** ：

| | 谁在做 | 做什么 | 在哪里 | 什么东西 |
|---|---|---|---|---|
| 猫 V | 1 | 0 | 0 | 0 |
| 坐 V | 0 | 1 | 0 | 0 |
| 垫 V | 0 | 0 | 1 | 1 |

新”猫” = 15%×猫V + **69%×坐V** + 15%×垫V = [0.15, **0.69**, 0.15, 0.15]

“猫”现在 69% 知道”在做什么”——因为从”坐”那里获取了最多信息！

----

## 注意力公式，一行就够

**Attention(Q, K, V) = softmax(QKᵀ / √d) · V**

拆开来看，刚才每一步都对应公式的一部分：

| 符号 | 对应哪一步 |
|------|------|
| QKᵀ | 点积——比较“成绩单” |
| / √d | 缩一缩——降降温 |
| softmax | 画饼图——变成百分比 |
| · V | 按比例混合——得到最终结果 |

一句话总结：**比成绩单、降温、画饼图、按比例混合** 。为什么一定要“降温”？下一页揭晓！

----

## 不除以 √d 会怎样？

我们用 d=4 算的时候分数还不大。但真正的大语言模型用 d=512，点积分数会变得 **非常大** ：

| | 猫 | 坐 | 垫 |
|---|---|---|---|
| 不缩放（d=512） | 12 | **98** | 15 |
| 画饼图后 | ≈0% | **≈100%** | ≈0% |

饼图几乎被“坐”一个 token 独占了！其他 token 的信息完全被忽略——大语言模型变成了“只看一个 token”。

----

## 除以 √d：让注意力更“柔和”

把分数先除以 √512 ≈ 22.6，“降降温”：

| | 猫 | 坐 | 垫 |
|---|---|---|---|
| 缩放后 | 0.5 | **4.3** | 0.7 |
| 画饼图后 | 2% | **93%** | 3% |

“坐”仍然拿到最大的一块饼，但“猫”和“垫”也能贡献一点信息。

这就是“从分数到百分比”里那一步“缩一缩”的原因——**防止大语言模型只盯着一个 token 看** 。

----

## 注意力计算流程图

![注意力计算流程](src/zh/images/attention-computation.svg)

----

## 多头注意力 (Multi-Head Attention)：多双眼睛同时看

一组 Query-Key-Value 可能只关注语法关系，那语义呢？位置呢？

- Transformer 用了 **多个注意力头** ——相当于同时有 8 双或 16 双眼睛
- 一双眼睛注意到“猫”是“坐”的主语
- 另一双眼睛注意到“垫子”跟在“在”后面
- 每双眼睛捕捉 **不同类型的关系**

合在一起，就给了大语言模型一个多层次、立体化的理解。

----

## 多头注意力内部

输入向量被 **切成 h 份** ，每一份独立做注意力，最后拼回来：

1. 假设输入维度 d=512，有 h=8 个头
2. 每个头只看 512÷8 = **64 个维度**
3. 8 个头各自独立算注意力
4. 最后把 8 个头的结果 **拼接** 起来，再做一次线性投影

> 就像一个班级分成 8 个小组讨论，每组关注一个角度，最后全班汇总。

----

## 为什么多头比单头好？

一个头必须把 **所有关系** 压缩进一组分数——负担太重了！

多头可以 **分工** ，每个头自动学会关注不同类型的关系：

以“小猫坐在垫子上”为例：

- **头 1（语法）** ：“猫”→“坐”（主语→动词）
- **头 2（位置）** ：“在”→“垫子”（介词→宾语）
- **头 3（语义）** ：“坐”→“垫子”（动作→地点）

合在一起，大语言模型就同时理解了语法、位置和语义——比任何单个头都全面得多。

----

## 但是，token 的顺序怎么办？

“狗咬人”和“人咬狗”里的 token 都一样，意思却天差地别！

- 注意力机制既然是同时看所有 token，那它怎么知道哪个 token 在前、哪个在后？
- 解决办法： **位置编码 (Positional Encoding)** — 给每个 token 编一个“座位号”

> 第 1 个 token：“狗”（1 号位）→ 第 2 个 token：“咬”（2 号位）→ 第 3 个 token：“人”（3 号位）

有了座位号，大语言模型就能同时知道“token 的含义”和“token 的顺序”。

----

## 位置编码怎么编？

大语言模型用 **正弦和余弦** 函数来给每个位置编码——不同频率的波叠加，就像钟表的指针：

- **秒针** （高频）— 变化很快，区分相邻位置
- **分针** （中频）— 变化较慢，区分远一些的位置
- **时针** （低频）— 变化最慢，区分很远的位置

每个位置都有一组独一无二的“波纹指纹”——不同速度的指针组合在一起，没有两个位置完全相同。

----

## 为什么用正弦余弦？

因为正弦余弦有一个神奇的性质：位置 pos+k 的编码，可以用位置 pos 的编码 **通过线性变换得到** 。

这意味着大语言模型可以轻松学会“相对距离”：

- “距离 3 个位置”在任何起点看起来都一样
- 训练时没见过位置 1000？没关系，波的规律是一样的

----

## 如果不用正弦余弦呢？

用简单编号（1, 2, 3...）行不行？

- 数字越来越大，超出训练范围就失效了
- 没法表达“相对距离”，只能表达“绝对位置”

> 正弦余弦编码 = 大语言模型理解“远近”的秘密武器

----

## 每一层里面有什么？

现在我们知道了注意力、多头注意力和位置编码，来看完整的一层 Transformer 包含什么：

1. **多头注意力** ：每个 token 同时扫一眼所有 token，多双眼睛捕捉不同关系
2. **前馈网络 (Feed-Forward Network)** ：每个 token 根据收集到的线索，独立“想一想”，更新自己的理解

此外还有一条 **捷径连接** ：把上一层的理解原封不动地传过来。即使某一层没学到新东西，也不会丢掉已有的知识。

每一层 = 观察 → 思考 → 带着更深的理解进入下一层。

----

## 捷径连接为什么重要？

想象 96 个人站成一排传话——传到最后，内容早就面目全非了。

这就是 **梯度消失** 问题：信号经过太多层之后越来越弱，大语言模型就“学不动”了。

怎么解决呢？

----

## 捷径连接的解决办法

> output = layer(x) + x

- 每一层把输入 **原样抄送一份** 到输出
- 即使某一层没学到有用的东西（layer(x) ≈ 0），原始信息也不会丢
- 就像传话游戏里，除了口耳相传，还同时发了一条原始短信

----

## 层归一化 (Layer Normalization)

捷径连接解决了信号丢失的问题，但还有一个麻烦：数字经过多层计算后可能变得 **忽大忽小** ，大语言模型就会“晕”。

**层归一化** 就像每层结束后做一次“校准”：

- 把所有数字重新调整到 **统一范围**
- 不改变数字之间的大小关系，只让它们更整齐
- 就像考完试把原始分换算成百分制——方便比较，也方便下一步计算

> 捷径连接也叫 **残差连接 (Residual Connection)** ——再加上层归一化，就是 Transformer 架构图中常见的”Add & Norm”模块。

----

## 同一个 token，层层蜕变

以“苹果”为例，看它在不同层之间怎么变：

- **刚输入时** ：只是一个符号，大语言模型还不知道它是什么意思
- **经过前几层** ：结合前后文，发现“苹果”跟“吃”或“公司”有关
- **“我吃了一个苹果”** → 经过深层：圆的、红的、甜的——是水果
- **“苹果发布了新手机”** → 经过深层：科技公司、iPhone、库克

同一个 token，最终的理解截然不同——这就是 12 层、24 层、甚至 96 层“变换”的力量。

----

## 一份完整的 Transformer “施工图”

![Transformer 完整架构](src/zh/images/transformer-architecture.svg)

----

<!-- .slide: class="center" -->

## Transformer 的架构搞清了，下一个问题——

*怎么训练它？*

----

## 学语言的两条路

想象你在学一门外语，有两种学法：

- **做完形填空** — 挖掉一个词让你猜，练的是*理解*能力
- **接龙写故事** — 给你开头，让你写下一句，练的是*生成*能力

研究人员把两条路都试了一遍！

----

## 路线一：完形填空（BERT）

> “小 ___ 坐在垫子上”

让大语言模型学着猜被挖掉的词，它就变得特别擅长 **理解** 文本。

- 强项：回答问题、给文本分类、信息检索
- 弱项：写故事、聊天这类生成任务

> [BERT](https://arxiv.org/abs/1810.04805)（2018）— 同时从左右两个方向理解上下文

----

## BERT 是怎么学习的？

想象你做了成千上万遍完形填空：

1. 拿一句话：“小猫坐在垫子上”
2. **遮住** 一个词：“小猫 xx 垫子上”
3. 大语言模型利用 **两边的线索** 来猜被遮住的词
4. 对完答案，从错误中学习，再来几百万遍

关键点：BERT 同时从 **左边和右边** 来理解上下文。就像看推理小说时已经知道了结局——你对每一条线索的理解都会更透彻。

----

## BERT 训练任务一：遮词猜词 (MLM)

BERT 其实有两项训练任务。第一项叫 **遮词猜词 (Masked Language Model)** ：

- 随机遮住 15% 的 token，让大语言模型猜
- “小猫 [MASK] 在垫子上” → 猜“坐”
- 这让大语言模型学会 **词级别** 的理解

就像老师在黑板上遮住几个字，让你根据上下文猜出来——猜得越多，理解力越强。

----

## BERT 训练任务二：下句预测 (NSP)

第二项任务叫 **下句预测 (Next Sentence Prediction)** ：

- 给两句话，判断第二句是不是真的跟在第一句后面
- ✅ “小猫很累了” → “它趴在垫子上睡着了”
- ❌ “小猫很累了” → “今天天气真好”
- 这让大语言模型学会 **句子级别** 的关系

两项任务合力，BERT 既能看懂单个字，也能看懂整个段落。

----

## 路线二：接龙猜词（GPT）

> “小猫坐在 ___”

让大语言模型学着预测下一个词，它就变得特别擅长 **生成** 文本。

- 强项：写作、对话、内容创作
- 弱项：需要深入理解全文的任务

> [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)（2018）— 从左到右逐词预测

----

## GPT 是怎么学习的？

想象你在帮别人接话：

1. 读到：“从前有一……”
2. 猜：“个”——没错！记住这个规律。
3. 读到：“从前有一个……”
4. 猜：“国王”——读过的故事越多，接话接得越准。

GPT 每次只往后生成一个词，而且只看 **前面已经出现的内容** 。这叫 **自回归生成 (Autoregressive Generation)** ——每个新词都受之前所有词的影响，就这样一点一点把故事拼出来。

----

## GPT 的因果遮罩

GPT 生成文本时有一个关键限制：**因果遮罩 (Causal Mask)** 。

每个位置 **只能看到前面的 token** ，看不到后面的：

> 位置 1：“从” → 只看到自己
> 位置 2：“前” → 看到“从”
> 位置 3：“有” → 看到“从”“前”

为什么要这样？因为生成时后面的 token 还没写出来——不能偷看还不存在的答案！

----

## 温度：控制创意的旋钮

GPT 选下一个词时，有一个旋钮叫 **温度 (Temperature)** ：

- **低温（0.1）** ：几乎总是选概率最高的词 → 安全、重复
- **高温（1.5）** ：让低概率的词也有机会被选中 → 创意、惊喜

就像考试：低温 = 只填最保险的答案；高温 = 敢猜冷门答案

----

## 能不能两全其美？

把理解和生成结合起来行不行？

> 思路：把 **所有任务** 都统一成“输入一段文字、输出一段文字”

- 翻译：“English: Hello” → “French: Bonjour”
- 摘要：“一篇长文章…” → “一段简短摘要…”
- 问答：“天空是什么颜色？” → “蓝色”

> [T5](https://arxiv.org/abs/1910.10683)（2019）— 一个大语言模型、一种格式，通吃所有任务

----

## 万能格式：更多例子

T5 的“文本到文本”思路能覆盖多少任务？看看这些例子：

- **情感分析** ：“classify: I love this movie” → “positive”
- **纠错** ：“fix: She don't like cats” → “She doesn't like cats”
- **闲聊** ：“respond: 你好呀” → “你好！有什么能帮你的吗？”

一个大语言模型就能搞定 **所有这些** 。不需要为每种任务单独设计架构——只要换一下开头的指令就行。

----

## 三条路的正式名字

把文字变成内部表示叫 **编码 (Encode)** ，把内部表示变回文字叫 **解码 (Decode)** 。

| 我们学过的 | 正式名称 | 看的方向 |
|---|---|---|
| 完形填空（BERT） | **Encoder-only** （只编码） | ← 左右都看 → |
| 接龙猜词（GPT） | **Decoder-only** （只解码） | 只看前文 → |
| 两全其美（T5） | **Encoder-Decoder** | 先编码，再解码 |

> GPT 的全称是 **Generative Pre-trained Transformer** ——一个只用 Decoder、专门用来 **生成** 的 Transformer。

----

## 更精巧的设计一定更好吗？

答案出人意料： **不一定** ！

研究人员发现，与其设计更复杂的大语言模型结构，不如把简单大语言模型 **训练得更充分** 、喂 **更多数据** ：

- 多练几轮，别急着停
- 多用新数据，少重复旧数据
- 训练方法比大语言模型设计更关键

> [RoBERTa](https://arxiv.org/abs/1907.11692)（2019）— 同样的 BERT 架构，靠训练技巧取胜

----

## 换个角度看

从宏观来看，大语言模型其实就是一个巨大的 **数学函数** ：

- **输入** ：一串 token
- **函数内部** ：词嵌入 → 注意力 → 前馈网络 → … → 重复 N 层
- **输出** ：下一个词的概率

所有的向量空间、点积、矩阵乘法、softmax——加在一起，就是一个函数：**f(输入文本) = 输出文本** 。几千亿个参数，做的事情本质上和 y = f(x) 一样。

----

## 第 1 章总结

| 论文 | 核心思想 |
|------|----------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762)（2017） | 同时看所有 token — Transformer 诞生 |
| [BERT](https://arxiv.org/abs/1810.04805)（2018） | 完形填空 → 擅长理解 |
| [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)（2018） | 接龙猜词 → 擅长生成 |
| [T5](https://arxiv.org/abs/1910.10683)（2019） | 一切任务 = 文本进、文本出 |
| [RoBERTa](https://arxiv.org/abs/1907.11692)（2019） | 训练方法比大语言模型设计更重要 |

----

## 想深入了解？

- [3Blue1Brown: But what is a GPT?](https://www.youtube.com/watch?v=wjZofJX0v4M) — 用精美动画讲解 Transformer 的工作原理
- [Jay Alammar: The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) — 用逐步图解拆解 Attention、Key、Query、Value
- [Andrej Karpathy: Let's build GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY) — 手把手从零搭一个迷你 GPT（视频，2 小时）
- [BERT Explained Visually](https://jalammar.github.io/illustrated-bert/) — 图解 BERT 是如何做完形填空的

----

## 想一想

- 如果让你来设计一个大语言模型，你会让它从左到右读（像 GPT），还是一次看全部（像 BERT）？为什么？
- 为什么“unhappiness”会被分成两个 token，而“happiness”却保持完整？你还能想到哪些可能被拆分的词？
- 你能想到一个 **理解** 比 **生成** 更重要的任务吗？反过来呢？
- RoBERTa 告诉我们训练方法比设计更重要。你的生活中有没有“努力练习胜过天赋”的例子？

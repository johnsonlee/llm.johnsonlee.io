<!-- .slide: class="center" -->

## 15. 量化与推理加速

*这些模型也太大了——我的电脑能跑吗？*

----

## 问题：模型大到装不下

GPT-3 有 1750 亿个参数，每个参数占 16 位存储。

- 加起来就是 **350 GB** ——相当于 350 部电影的大小，光加载就这么多！
- 普通 GPU 只有 24 GB 显存
- 跑起来需要一整个昂贵的服务器集群

> 能不能在不明显变笨的前提下，把模型 **瘦身** ？

----

## 量化 (Quantization)：四舍五入的艺术

想象一下 **3.14159265…** 这个数：

- 全精度（16 位）：3.14159265
- 8 位：3.14
- 4 位：3.1
- 精度降了，但占用空间 **小了好几倍**

GPTQ 把这个“四舍五入”的思路用到全部 1750 亿个数字上，模型一下子缩小了 4 倍！

> [GPTQ](https://arxiv.org/abs/2210.17323)（2022）— 3-4 位量化，效果几乎不打折

----

## AWQ：分清轻重缓急

GPTQ 对所有参数一视同仁地压缩，但其实有些参数 **比别的更重要** ！

- 少数 **特别重要的数字** 承载了模型的大部分核心能力
- AWQ 的策略：重点保护这些关键权重，其余的放心大胆压缩
- 同样的压缩比，效果却更好

> [AWQ](https://arxiv.org/abs/2306.00978)（2023）— 重要的权重优先保护

----

## vLLM：同时伺候成千上万的用户

模型瘦身了，但 **几千人同时访问** 时还是扛不住：

- 每个对话都需要独立的键值缓存，占显存
- 很多缓存只用了一半就被浪费了
- vLLM 的办法：像操作系统管理内存一样，用 **分页机制** 来管缓存

> [vLLM](https://arxiv.org/abs/2309.06180)（2023）— PagedAttention，如今推理服务的标配

----

## 推测解码 (Speculative Decoding)：先猜后验

逐词生成文本很慢，因为每个词都要等前一个词出来才能算。

能不能先用一个 **又小又快的模型** 抢先猜几个词？

1. 小模型飞快地猜出 5 个词
2. 大模型 **一次性** 验证这 5 个词（这一步很快）
3. 猜对了 → 全部采纳，速度直接提升好几倍
4. 猜错了 → 从出错处改正，继续往前

> [Speculative Decoding](https://arxiv.org/abs/2211.17192)（2022）— 小模型打草稿，大模型来审批

----

## Medusa：同时预测好几个词

如果让模型自己一口气预测 **后面好几个词** 呢？

- 给模型加装多个“预测头”
- 每个头同时预测不同位置的未来词
- 批量验证并采纳正确的预测

就像下棋时往后多想几步。

> [Medusa](https://arxiv.org/abs/2401.10774)（2024）— 多头并行预测，加速输出

----

## 第 15 章总结

| 论文 | 核心思想 |
|------|----------|
| [GPTQ](https://arxiv.org/abs/2210.17323)（2022） | 压缩到 3-4 位，体积缩小 4 倍 |
| [AWQ](https://arxiv.org/abs/2306.00978)（2023） | 压缩时优先保护关键权重 |
| [vLLM](https://arxiv.org/abs/2309.06180)（2023） | 分页式显存管理，高并发服务 |
| [Speculative Decoding](https://arxiv.org/abs/2211.17192)（2022） | 小模型打草稿，大模型验证 |
| [Medusa](https://arxiv.org/abs/2401.10774)（2024） | 多头并行预测，一次出好几个词 |

<!-- .slide: class="center" -->

## 15. 量化与推理加速

*这些大语言模型也太大了——我的电脑能跑吗？*

----

## 问题：大语言模型大到装不下

GPT-3 有 1750 亿个参数，每个参数占 16 位存储。

- 加起来就是 **350 GB** ——相当于 350 部电影的大小，光加载就这么多！
- 普通 GPU 只有 24 GB 显存
- 跑起来需要一整个昂贵的服务器集群

> 能不能在不明显变笨的前提下，把大语言模型 **瘦身** ？

----

## 量化 (Quantization)：四舍五入的艺术

想象一下 **3.14159265…** 这个数：

- 全精度（16 位）：3.14159265
- 8 位：3.14
- 4 位：3.1
- 精度降了，但占用空间 **小了好几倍**

GPTQ 把这个“四舍五入”的思路用到全部 1750 亿个数字上，大语言模型一下子缩小了 4 倍！

> [GPTQ](https://arxiv.org/abs/2210.17323)（2022）— 3-4 位量化，效果几乎不打折

----

## 质量损失有多大？

出乎意料地小！来看对比：

| 精度 | 大语言模型大小 | 质量（对比原始） |
|------|---------|-----------------|
| 16 位（原始） | 350 GB | 100% |
| 8 位 | 175 GB | 约 99% |
| 4 位 (GPTQ) | 88 GB | 约 97% |
| 3 位 | 66 GB | 约 93% |

从 16 位降到 4 位，大语言模型缩小了 **4 倍** ，却保留了 97% 的智力。就像把蓝光电影压缩成流媒体画质——几乎看不出区别！

----

## AWQ：分清轻重缓急

GPTQ 对所有参数一视同仁地压缩，但其实有些参数 **比别的更重要** ！

- 少数 **特别重要的数字** 承载了大语言模型的大部分核心能力
- AWQ 的策略：重点保护这些关键权重，其余的放心大胆压缩
- 同样的压缩比，效果却更好

> [AWQ](https://arxiv.org/abs/2306.00978)（2023）— 重要的权重优先保护

----

## vLLM：同时伺候成千上万的用户

大语言模型瘦身了，但 **几千人同时访问** 时还是扛不住：

- 每个对话都需要独立的键值缓存，占显存
- 很多缓存只用了一半就被浪费了
- vLLM 的办法：像操作系统管理内存一样，用 **分页机制** 来管缓存

> [vLLM](https://arxiv.org/abs/2309.06180)（2023）— PagedAttention，如今推理服务的标配

----

## PagedAttention：为什么它很重要

没有 PagedAttention 时，为大语言模型提供服务会浪费大量显存：

- 每个用户的对话都会预留一整块固定的内存——即便大部分是空的
- 就像一个人也要预订整张餐桌

有了 PagedAttention：
- 显存被切成一小页一小页（就像书页一样）
- 每段对话只占用它实际需要的页数
- 相似的对话之间还可以共享页面！

效果：同样的硬件可以 **多服务 2-4 倍的用户** 。

----

## 推测解码 (Speculative Decoding)：先猜后验

逐词生成文本很慢，因为每个词都要等前一个词出来才能算。

能不能先用一个 **又小又快的大语言模型** 抢先猜几个词？

1. 小模型飞快地猜出 5 个词
2. 大模型 **一次性** 验证这 5 个词（这一步很快）
3. 猜对了 → 全部采纳，速度直接提升好几倍
4. 猜错了 → 从出错处改正，继续往前

> [Speculative Decoding](https://arxiv.org/abs/2211.17192)（2022）— 小模型打草稿，大模型来审批

----

## 推测解码：一个生活中的类比

想象一个老板和助理一起写报告：

- **没有推测** ：老板逐字口述，助理写一个词、等一个词。太慢了！
- **有了推测** ：助理先飞快地起草好几段。老板快速扫一眼，大部分直接批准，只改有问题的地方。快多了！

小号“助理”大语言模型比大号“老板”大语言模型快 10-100 倍。当助理猜对了（大约 70-80% 的概率），我们就省掉了所有慢速生成的时间！

----

## Medusa：同时预测好几个词

如果让大语言模型自己一口气预测 **后面好几个词** 呢？

- 给大语言模型加装多个“预测头”
- 每个头同时预测不同位置的未来词
- 批量验证并采纳正确的预测

就像下棋时往后多想几步。

> [Medusa](https://arxiv.org/abs/2401.10774)（2024）— 多头并行预测，加速输出

----

## 第 15 章总结

| 论文 | 核心思想 |
|------|----------|
| [GPTQ](https://arxiv.org/abs/2210.17323)（2022） | 压缩到 3-4 位，体积缩小 4 倍 |
| [AWQ](https://arxiv.org/abs/2306.00978)（2023） | 压缩时优先保护关键权重 |
| [vLLM](https://arxiv.org/abs/2309.06180)（2023） | 分页式显存管理，高并发服务 |
| [Speculative Decoding](https://arxiv.org/abs/2211.17192)（2022） | 小模型打草稿，大模型验证 |
| [Medusa](https://arxiv.org/abs/2401.10774)（2024） | 多头并行预测，一次出好几个词 |

----

## 想深入了解？

- [A Visual Guide to Quantization (Maarten Grootendorst)](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization) — 用精美图表讲解量化
- [vLLM: Easy and Fast LLM Serving (blog)](https://blog.vllm.ai/) — PagedAttention 底层原理揭秘
- [Running LLMs Locally (Ollama)](https://ollama.com/) — 在自己的电脑上跑一个量化模型试试
- [Speculative Decoding Explained (Hugging Face)](https://huggingface.co/blog/whisper-speculative-decoding) — “先猜后验”如何加速生成

----

## 想一想

- 量化就像四舍五入——3.14159 变成 3.1。你能想到其他“差不多就行”的场景吗？在哪些情况下四舍五入反而会出问题？
- vLLM 的 PagedAttention 借鉴了操作系统管理内存的思路。你觉得计算机科学里还有哪些概念可以拿来给大语言模型加速？
- 推测解码用小模型来猜大模型会说什么。如果小模型猜得很差呢？还能加速吗？
- 如果量化能让 350 GB 的大语言模型缩到 88 GB，哪些原本跑不动大语言模型的设备现在可以跑了？这会带来什么新的可能？
